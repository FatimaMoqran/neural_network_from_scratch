{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_Xor(nb_features=400):\n",
    "  \n",
    "    \"\"\"\n",
    "        Function that creates XOR dataset\n",
    "        :param nb_features: number of features\n",
    "        :type nb_features : int\n",
    "        :return: X  input numpy array shape(2,400)\n",
    "        :rtype: numpy array\n",
    "        :return: Y true-false labels numpy array shape(400,1)\n",
    "        :rtype: numpy array\n",
    "    \n",
    "      \"\"\"\n",
    "    X = np.random.randint(2, size = (2,nb_features))\n",
    "    Y = np.logical_xor(X[0,:],X[1,:])\n",
    "    print(Y)\n",
    "  \n",
    "    #reshape Y\n",
    "  \n",
    "    Y = Y.reshape(1,400)\n",
    "                        \n",
    "    print(f\"X.shape = {X.shape}\")\n",
    "    print(f\"Y.shape = {Y.shape}\")\n",
    "                        \n",
    "    return X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ True False False False  True  True False False False  True  True  True\n",
      " False  True  True False False False False  True False  True False False\n",
      "  True  True  True  True False  True False False False False  True False\n",
      "  True False  True  True  True  True  True False False False  True  True\n",
      " False False False False False False False False False  True  True False\n",
      " False  True  True False False False False False  True  True False False\n",
      " False  True  True  True False  True False  True False  True False False\n",
      " False  True  True False False  True  True  True  True  True False  True\n",
      "  True  True False False  True  True  True  True  True  True False False\n",
      "  True False  True False False  True  True False  True  True  True  True\n",
      " False  True False  True False  True False  True  True False  True  True\n",
      " False False  True  True False  True False  True False  True  True  True\n",
      " False  True False False  True False False False  True False  True False\n",
      " False False  True False False False  True False False False  True  True\n",
      "  True False False False False  True False  True  True  True False  True\n",
      "  True  True False  True  True  True  True  True  True False False  True\n",
      "  True  True False  True False  True False  True False  True  True  True\n",
      "  True False  True  True  True False  True  True False False False  True\n",
      " False False False False  True  True  True  True False False  True  True\n",
      "  True False False  True  True False False False False False  True False\n",
      "  True  True  True  True  True  True  True  True  True  True False  True\n",
      "  True  True False  True False  True False False  True False False  True\n",
      "  True  True False  True  True False False False False  True False False\n",
      "  True  True  True False False  True  True  True  True  True  True False\n",
      " False  True  True  True False  True  True  True False  True  True  True\n",
      "  True False False  True False False False False False False  True False\n",
      "  True  True False  True False False False False False  True  True  True\n",
      " False False  True False False  True False False  True False  True  True\n",
      " False  True False  True False  True False False  True  True False False\n",
      "  True False  True False False False  True False  True False  True  True\n",
      "  True  True False False False False False False  True  True  True False\n",
      "  True False  True  True False  True False False False  True False  True\n",
      "  True False  True  True False  True False  True False  True  True  True\n",
      " False False False  True]\n",
      "X.shape = (2, 400)\n",
      "Y.shape = (1, 400)\n"
     ]
    }
   ],
   "source": [
    " X,Y = create_Xor()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "  \"\"\"\n",
    "      Function that implement sigmoid\n",
    "      :param Z: input value\n",
    "      :type Z: float, array\n",
    "      :return s: sigmoid result of Z \n",
    "      :rtype s:  float or array\n",
    "  \"\"\"\n",
    "  \n",
    "  s = 1/(1+(np.exp(-Z)))\n",
    "  # sigmoid(0) == 0.5\n",
    "  \n",
    "  assert(0.5 == 1/(1+(np.exp(0))))\n",
    "  \n",
    "  return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8807970779778823"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test sigmoid\n",
    "\n",
    "sigmoid(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5       , 0.88079708])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sigmoid d'un array\n",
    "sigmoid_array= sigmoid(np.array([0,2]))\n",
    "sigmoid_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "    \"\"\"\n",
    "          Function that implement relu\n",
    "          :param Z: input value\n",
    "          :type Z: float, array\n",
    "          :return: relu result of Z \n",
    "          :rtype: float or array\n",
    "      \"\"\"\n",
    "  \n",
    " #   Z>0 c'est un test si Z>0 alors on va avoir 1 sinon on va avoir 0\n",
    "\n",
    "    r = Z * (Z>0)\n",
    "    \n",
    "    #r = np.maximum(0,Z)\n",
    "   \n",
    "    assert(0.5 == 0.5 * (0.5>0))\n",
    "    assert(0   == -1  * (-1>0))\n",
    "  \n",
    "  \n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivative_sigmoid(dA,activation_cache):\n",
    "    \"\"\"\n",
    "      Function that implement the derivative of sigmoid\n",
    "      :param Z: input value\n",
    "      :type Z: float, array\n",
    "      :return: derivative result of Z \n",
    "      :rtype: float or array\n",
    "    \"\"\"\n",
    "    Z = activation_cache\n",
    "#     print(\"Z.shape = \", Z.shape, \"dA.shape = \", dA.shape)\n",
    "    s = 1/(1+np.exp(-Z))\n",
    "    dZ= dA * s *(1-s)\n",
    "#     print(\"dZ.shape = \", dZ.shape, \"s.shape = \", s.shape)\n",
    "    assert(0.25 == sigmoid (0) * (1-sigmoid(0)))\n",
    "  \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dZ = derivative_sigmoid(45)\n",
    "# dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def derivative_relu(dA,activation_cache):\n",
    "#   \"\"\"\n",
    "#       Function that implement the derivative of relu\n",
    "#       :param Z: input value\n",
    "#       :type Z: float, array\n",
    "#       :return: derivative result of Z \n",
    "#       :rtype: float or array\n",
    "      \n",
    "#   \"\"\"\n",
    "  \n",
    "#   dZ= 1 * (Z>0)\n",
    "  \n",
    "#   assert (0 == 1 * (-1>0) )\n",
    "#   assert (1 == 1 * (1>0) )\n",
    "  \n",
    "#   return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivative_relu(dA, activation_cache):\n",
    "    \n",
    "    Z = activation_cache\n",
    "    dZ = np.array(dA,copy = True)\n",
    "    dZ[Z<= 0] = 0\n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dZ= derivative_relu(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialisation_deep(dim_layers):\n",
    "  \n",
    "    \"\"\" \n",
    "      Function that initialize weights and biais for each layer\n",
    "      :param dim_layers: list of each layer\n",
    "      :type dim_layers: pyhon list\n",
    "      :return parameters dictionnary with W1,b1,......WL,bL\n",
    "      :rtype: python dictionnary\n",
    "      Wl --- weight matrix of shape (dim_layers[l],dim_layers[1-l])\n",
    "      b1 --- weight matrix of shape (dim_layer[l],dim_layers[1-l])\n",
    "    \"\"\"\n",
    "    np.random.seed(3)\n",
    "    parameters={}\n",
    "    \n",
    "    L = len(dim_layers)\n",
    "    \n",
    "    for l in range(1,L):\n",
    "   \n",
    "        parameters[f'W{l}']= np.random.randn(dim_layers[l], dim_layers[l-1])*0.01\n",
    "\n",
    "        parameters[f'b{l}'] = np.zeros((dim_layers[l],1))\n",
    "    \n",
    "#     print(f'W{l}.shape = {dim_layers[l]},{dim_layers[l-1]}')\n",
    "#     print(f\"b{l}.shape = {dim_layers[l]}, 1 \")\n",
    "\n",
    "    assert (parameters[f\"W{l}\"].shape == (dim_layers[l],dim_layers[l-1]))\n",
    "    assert (parameters[f\"b{l}\"].shape == (dim_layers[l],1))\n",
    "    \n",
    "    return parameters\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'W1': array([[ 0.01788628,  0.0043651 ],\n",
       "        [ 0.00096497, -0.01863493],\n",
       "        [-0.00277388, -0.00354759]]), 'b1': array([[0.],\n",
       "        [0.],\n",
       "        [0.]]), 'W2': array([[-0.00082741, -0.00627001, -0.00043818]]), 'b2': array([[0.]])}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim_layers= [2,3,4,5,1]\n",
    "parameters = initialisation_deep(dim_layers)\n",
    "\n",
    "parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_forward (A_prev ,W , b, activation):\n",
    "    \"\"\"\n",
    "      function that both computes preactivation and activation \n",
    "      :param A_prev: previews activation matrix (for the first layer it is X )\n",
    "      :param W: weight matrix for the current layer\n",
    "      :param b: biais matrix for the current layer\n",
    "      :param activation: choice of activation function eg: sigmoid , relu\n",
    "      :type A_prev : matrix of float\n",
    "      :type W: matrix of float\n",
    "      :type b: matrix of float\n",
    "      :return: A matrix of activation\n",
    "      :cache: tuple of (linear_cache,activation_cache) \n",
    "      \"\"\"\n",
    "\n",
    "    Z = np.dot(W,A_prev)+b\n",
    "\n",
    "    linear_cache = (A_prev,W,b)\n",
    "  \n",
    "  \n",
    "    assert(Z.shape == (W.shape[0], A_prev.shape[1]))\n",
    "  \n",
    "    if activation == \"sigmoid\":\n",
    "         \n",
    "        A = sigmoid(Z)\n",
    "        activation_cache = A\n",
    "    \n",
    "    elif activation == \"relu\":\n",
    "         \n",
    "        A = relu(Z)\n",
    "        activation_cache = A\n",
    "         \n",
    "         \n",
    "    assert (A.shape == Z.shape)\n",
    "         \n",
    "    cache = (linear_cache, activation_cache)\n",
    "  \n",
    "    return A,cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A,cache = linear_activation_forward(X,parameters['W1'],parameters['b1'],\"relu\")\n",
    "# print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # A_test= np.array([\n",
    "# #            [3,4,5],\n",
    "# #            [3,4,7],\n",
    "#            [4,2,1]\n",
    "#                   ])\n",
    "# # W_test= np.array([\n",
    "#                 [5,3,1],\n",
    "#                 [6,7,9],\n",
    "#                 [3,5,2]\n",
    "#                        ])\n",
    "# # b_test= ([\n",
    "#           [0],\n",
    "#           [0],\n",
    "#           [0]\n",
    "#              ])\n",
    "# # A,cache = linear_activation_forward(A_test,W_test,b_test,\"relu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# faire une fonction forward sur l layers\n",
    "def forward_layers(X,parameters):\n",
    "    \"\"\"\n",
    "       Function that computes the forward activation for L layers\n",
    "      :param X: input matrix, shape (input_size, number of exemple)\n",
    "      :param parameters: output of initialisation_deep dictionnary of W, b\n",
    "      :type X: matrix of float\n",
    "      :type parameters : dictionary of matrices\n",
    "      :return AL : last post activation value\n",
    "      :return caches : list of caches with every caches of linear_activation_forward \n",
    "      :rtype AL: matrix of float\n",
    "      :rtype caches: list of tuples\n",
    "\n",
    "  \"\"\"\n",
    "    #je crée une  liste de  caches où je vais stoker les valeurs obtenues\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2 # je calcul le nombre de couches par rapport aux nombres de paramètres\n",
    "    #je fais une boucle pour toutes les couches jusqu'à L-1\n",
    "    for l in range (1,L): \n",
    "    #je considère X comme \n",
    "        A_prev= A\n",
    "#       je fais mon calcul pour A0 jusqu'à L-1 ou bien (1 à L)\n",
    "        A,cache = linear_activation_forward(A_prev,parameters[f'W{l}'],parameters[f'b{l}'],\"sigmoid\")\n",
    "       \n",
    "        #je rajoute le cache obtenu dans la liste cache\n",
    "        caches.append(cache)\n",
    "      \n",
    "      #calcul pour la dernière couche:\n",
    "      #je récupère le dernier A qui est sorti de mes couches précédentes et je lui mets une sigmoid \n",
    "      \n",
    "    AL,cache = linear_activation_forward(A,parameters[f'W{l+1}'],parameters[f'b{l+1}'],\"sigmoid\")\n",
    "    caches.append(cache)\n",
    "   \n",
    "    assert AL.shape == (1,X.shape[1])\n",
    "    \n",
    "    return AL,caches\n",
    "\n",
    " \n",
    "  \n",
    "  \n",
    "      \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AL,caches = forward_layers(X,parameters)\n",
    "# AL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #test sur les petits array\n",
    "\n",
    "# parameters_test={\"W1\":W_test,\n",
    "#                  \"b1\":b_test}\n",
    "\n",
    "# ALtest,caches_test = forward_layers(A_test,parameters_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = X.shape[1]\n",
    "\n",
    "def compute_cost(AL,Y):\n",
    "    \"\"\"\n",
    "        Function that compute the cost \n",
    "        :param AL: probability vector - shape (1,number of examples)\n",
    "        :param Y:  matrix of float\n",
    "        :type AL: matrix\n",
    "        :type Y: array of booleen\n",
    "        :return cost: cost result\n",
    "        :rtype: float \n",
    "    \"\"\"\n",
    "    #je calcule d'abord llog\n",
    "    logprob = (Y * np.log(AL) + (1-Y) * np.log(1-AL))\n",
    "    #ensuite la cost\n",
    "    cost = -(np.sum(logprob))/m\n",
    "    #je veux que l'on me retourne un nombre et non pas un array\n",
    "    cost = np.squeeze(cost)\n",
    "    #être sur que j'ai la cost au bon format\n",
    "    assert(isinstance(cost,float))\n",
    "  \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cost_test = compute_cost(Y,Y)\n",
    "# print(cost_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#backward L layer\n",
    "def linear_backward(dZ,cache):\n",
    "    \"\"\"\n",
    "        function that computes the linear backward\n",
    "        :param dZ: gradient of the cost with respect to linear output\n",
    "        :param cache: tuple of value\n",
    "        :return dA_prev: gradient of the cost with respect to activation\n",
    "        :return dW: gradient of the cost with respect to W\n",
    "        :return db: gradient of the cost with respect to b\n",
    "    \"\"\"\n",
    "    #recuperation des valeurs dont j'ai besoin\n",
    "    \n",
    "    A_prev, W, b = cache\n",
    "    m=A_prev.shape[1]\n",
    "    #calcul des dérivées:\n",
    "  \n",
    "    dW = np.dot(dZ,A_prev.T)/m\n",
    "    db = np.sum(dZ,axis=1,keepdims = True)/m\n",
    "    dA_prev= np.dot(W.T,dZ)\n",
    "    \n",
    "    assert(dA_prev.shape == A_prev.shape)\n",
    "    assert(dW.shape == W.shape)\n",
    "    assert(db.shape == b.shape)\n",
    "  \n",
    "    return dA_prev,dW,db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fonction backward pour l'implémentation de la backward si fonction d'activation est une sigmoid ou fonction d'activation est une RELU\n",
    "\n",
    "def linear_activation_backward(dA,cache,activation):\n",
    "    \"\"\"  \n",
    "        function that computes the linear activation backward with sigmoid and relu\n",
    "        :param dA: gradient of the activation for the current layer the l layer\n",
    "        :param cache: tuple of values with the parameters\n",
    "        :param activation: activation function-Relu or Sigmoid\n",
    "        :type dA: numpy array\n",
    "        :type activation: string\n",
    "        :return dA_prev: gradient activation of the l-1 layer-shape = A_prev shape\n",
    "        :return dW: gradient of the cost with respect of the W for the current layer l\n",
    "        :return db: gradient of the cost with respect of the b for the current layer l\n",
    "    \"\"\"\n",
    "    #je récupère de mon cache les paramètres \n",
    "\n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    # je calcule dZ en fonction de l'activation RELU ou Sigmoid\n",
    "    if activation == 'relu':\n",
    "        dZ = derivative_relu(dA, activation_cache)\n",
    "        dA_prev,dW,db = linear_backward (dZ,linear_cache)\n",
    "        \n",
    "    elif activation == 'sigmoid':\n",
    "        dZ = derivative_sigmoid(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ,linear_cache)\n",
    "    \n",
    "    return dA_prev, dW, db\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l_model_backward(AL,Y,caches):\n",
    "    \n",
    "    \"\"\" \n",
    "          function that compute the backward propagation\n",
    "          :param AL: array with the last activation -probability vector \n",
    "          :param Y: array with the label\n",
    "          :param caches: list of caches of all the parameters of relu activation and one cache with all the parameters with sigmoid\n",
    "          :type AL: numpy array\n",
    "          :type Y: vectord\n",
    "          :type caches: python list\n",
    "          :return grads: dictionnary of gradients dW,db,dA\n",
    "          \n",
    "    \"\"\"\n",
    "    grads = {}\n",
    "#     print(\"initialisation de grad\")\n",
    "    L = len(caches) #nombre de couches (correspond aux nombres de caches)\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape) #on reshape Y comme AL pour pouvoir faire les opérations\n",
    "#     print(\"L = \", L, \" m = \",m, \" Y = \",Y)\n",
    "    #     initialisation de la back propagation pour calculer dAL\n",
    "   \n",
    "    \n",
    "    dAL = -(np.divide(Y,AL)-np.divide(1-Y,1-AL))\n",
    "#     print(\"dAL=\", dAL)\n",
    "    \n",
    "  \n",
    "      #calcul des gradients pour la dernière couche L sigmoid \n",
    "    #j'utilise le cache de la dernière couche et je mets tout dans un dictionnaire\n",
    "\n",
    "    current_cache = caches[L-1]\n",
    "#     print(\"current_cache = \", current_cache)\n",
    "    grads[f\"dA{L-1}\"], grads[f\"dW{L}\"], grads[f\"db{L}\"] = linear_activation_backward(dAL,current_cache, activation =\"sigmoid\")\n",
    "#     print([f\"dA{L-1}\"], grads[f\"dW{L}\"], grads[f\"db{L}\"])\n",
    "  \n",
    "    #ensuite je fais une boucle pour les autres couches de l= l-2 à l = 0\n",
    "  \n",
    "    for l in reversed(range(L-1)):\n",
    "        \"\"\" \n",
    "#             entrée : la dérivée dA l+1 et le cache de la couche current\n",
    "#             sortie : la dérivée dA l et dWl+1 et dbl+1\n",
    "        \n",
    "#         \"\"\"\n",
    "        \n",
    "        current_cache = caches[l]\n",
    "        \n",
    "#         print(\"l = \", l , \"current_cache W =\", current_cache[0][1].shape)\n",
    "#         #je crée des variables temporaires: dA_prev_temp, dW_temp, db_temp\n",
    "    \n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward (grads[f\"dA{l+1}\"], current_cache, activation = \"sigmoid\")\n",
    "    \n",
    "# #         #je les mets dans le dictionnaire\n",
    "    \n",
    "        grads[f\"dA{l}\"] = dA_prev_temp\n",
    "        grads[f\"dW{l+1}\"] = dW_temp\n",
    "        grads[f\"db{l+1}\"] = db_temp\n",
    "\n",
    "#         print (\"da\", l, grads[f\"dA{l}\"].shape)\n",
    "\n",
    "        \n",
    "    return grads\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters,grads,learning_rate):\n",
    "    \"\"\"\n",
    "        function that update parameters using the gradient descent\n",
    "        :argument parameters: python dictionary with parameters\n",
    "        :argument grads: python dictionnary with all the gradient\n",
    "        :return parameters: python dictionnar with the updated parameters\n",
    "    \"\"\"\n",
    "    L = len (parameters)//2\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * grads[\"dW\" + str(l+1)]\n",
    "            \n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * grads[\"db\" + str(l + 1)]\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fonction globale qui compute tout (on commence pour faire la forward)\n",
    "\n",
    "def L_layer_model(X,Y, dim_layers, learning_rate = 0.0075,num_iterations = 3000, print_cost = False):\n",
    "    \n",
    "    \"\"\"\n",
    "        :param X: matrix of inputs\n",
    "        :param Y: vector of label\n",
    "        :param layers_dims: list that contains the input size and each layer size\n",
    "        :param learningrate: learning rate for the gradient descent\n",
    "        :param num_iterations: number of iterations of the loop\n",
    "        :param print_cost: decide if it print the cost (True)or not (False) \n",
    "        :type X: numpy matrix\n",
    "        :type Y: numpy array\n",
    "        :type layers_dims: python list\n",
    "        :type learningrate float\n",
    "        :type num_iteration: int\n",
    "        :type printcost : bool\n",
    "        :return \n",
    "    \"\"\"\n",
    "    np.random.seed(1)\n",
    "    costs = []\n",
    "    \n",
    "    #initialisation des paramètres\n",
    "    \n",
    "    parameters = initialisation_deep(dim_layers)\n",
    "    \n",
    "    #boucle de 0 à nombre d'iterations:\n",
    "    \n",
    "    for i in range (0,num_iterations):\n",
    "        \n",
    "        #forward propagation l layers\n",
    "        AL,caches = forward_layers(X,parameters)\n",
    "        \n",
    "        #compute cost\n",
    "        cost = compute_cost(AL,Y)\n",
    "#         print(cost)\n",
    "        \n",
    "        #L model backward\n",
    "        grads = l_model_backward(AL,Y,caches)\n",
    "        \n",
    "        #update parameters\n",
    "        parameters = update_parameters(parameters,grads,learning_rate)\n",
    "        \n",
    "    # Print the cost every 100 training example\n",
    "        if print_cost and i % 1000 == 0:\n",
    "            print(cost)\n",
    "#             print (f\"Cost after iteration {i}{cost}\")\n",
    "        if print_cost and i % 1000 == 0:\n",
    "            costs.append(cost)\n",
    "                \n",
    "#     plot the cost\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per hundreds)')\n",
    "    plt.title(f\"Learning rate ={learning_rate}\")\n",
    "    plt.show()\n",
    "\n",
    "    return parameters\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6932149186602016\n",
      "0.6924248476822709\n",
      "0.6917598506337087\n",
      "0.6918203874158085\n",
      "0.6923208323324785\n",
      "0.6920008996401111\n",
      "0.6916178550844596\n",
      "0.6930156745712877\n",
      "0.6014595866145068\n",
      "0.21660590186208345\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "parameters = L_layer_model(X,Y,dim_layers,learning_rate = 1.4 ,num_iterations = 10000, print_cost = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "gist": {
   "data": {
    "description": "neural_l_layerfromscratchV2.ipynb",
    "public": false
   },
   "id": ""
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
