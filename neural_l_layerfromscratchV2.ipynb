{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_Xor(nb_features=400):\n",
    "  \n",
    "    \"\"\"\n",
    "        Function that creates XOR dataset\n",
    "        :param nb_features: number of features\n",
    "        :type nb_features : int\n",
    "        :return: X  input numpy array shape(2,400)\n",
    "        :rtype: numpy array\n",
    "        :return: Y true-false labels numpy array shape(400,1)\n",
    "        :rtype: numpy array\n",
    "    \n",
    "      \"\"\"\n",
    "    X = np.random.randint(2, size = (2,nb_features))\n",
    "    Y = np.logical_xor(X[0,:],X[1,:])\n",
    "    print(Y)\n",
    "  \n",
    "    #reshape Y\n",
    "  \n",
    "    Y = Y.reshape(1,400)\n",
    "                        \n",
    "    print(f\"X.shape = {X.shape}\")\n",
    "    print(f\"Y.shape = {Y.shape}\")\n",
    "                        \n",
    "    return X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ True False  True False  True False False False  True  True  True False\n",
      " False  True False  True False  True False False  True False False  True\n",
      " False False False  True False  True False  True  True  True False False\n",
      "  True  True False False  True  True  True  True  True False False  True\n",
      " False  True  True False False False False False  True  True False False\n",
      " False  True  True False  True  True  True  True False  True  True  True\n",
      " False False  True False  True False  True  True False False  True False\n",
      " False  True False  True False  True False  True  True  True  True False\n",
      " False  True False False  True False  True  True False False  True  True\n",
      " False  True False  True  True False  True False False  True  True  True\n",
      " False False  True  True False False  True  True  True  True  True  True\n",
      "  True False  True False False  True  True False False False False  True\n",
      "  True  True  True  True  True False  True  True False  True  True  True\n",
      "  True  True  True  True False False  True  True  True  True  True  True\n",
      " False  True False False False  True  True False  True  True False False\n",
      " False False False False  True False  True  True False False False False\n",
      "  True False  True False False False  True False  True  True False False\n",
      " False  True  True  True False  True False False False  True False  True\n",
      " False False False  True False  True False  True  True  True False False\n",
      " False  True  True  True False False  True False False False False False\n",
      "  True False False False  True False False  True False  True False  True\n",
      " False False  True  True  True False  True  True  True  True  True False\n",
      " False False False False  True  True False False  True  True  True  True\n",
      " False False False False  True False  True False  True False False False\n",
      " False False  True False False False  True False False False  True  True\n",
      " False  True False  True  True  True  True  True  True False  True  True\n",
      "  True  True False  True  True False False  True  True False False  True\n",
      " False  True  True  True False  True False  True False  True False False\n",
      "  True False  True  True  True  True False False False False False False\n",
      "  True  True False  True  True False  True  True False  True  True  True\n",
      "  True  True  True False False  True False  True False  True  True False\n",
      "  True False  True False False False False False False False False  True\n",
      "  True False False  True  True False  True  True False  True  True False\n",
      "  True  True False False]\n",
      "X.shape = (2, 400)\n",
      "Y.shape = (1, 400)\n",
      "[[0 0 0 1 1 1 1 1 0 1 0 0 1 0 0 1 1 0 1 1 0 0 1 0 1 1 1 1 0 1 0 0 1 0 0 1\n",
      "  1 1 0 0 1 0 0 1 0 1 0 0 0 0 0 1 0 1 0 0 1 1 0 1 1 0 0 1 1 1 0 1 0 1 1 0\n",
      "  0 1 0 1 1 0 0 0 0 0 0 1 0 1 1 1 0 1 1 1 1 0 1 1 0 0 1 1 0 0 1 1 0 0 1 1\n",
      "  0 0 1 1 1 0 1 0 1 0 0 1 0 1 0 1 0 0 1 1 1 1 1 1 0 0 1 1 0 1 0 0 1 0 0 1\n",
      "  1 0 0 1 0 1 1 0 1 1 0 1 1 1 1 0 0 1 1 0 1 1 1 1 0 1 0 0 1 1 1 1 1 1 0 1\n",
      "  1 1 0 1 1 0 1 0 0 1 0 1 1 0 0 1 0 1 0 1 1 1 0 0 1 1 0 1 1 0 1 1 0 1 1 1\n",
      "  1 1 0 1 0 1 0 1 0 0 1 1 0 1 0 1 1 1 1 0 1 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
      "  1 1 0 0 0 1 0 1 0 0 0 0 0 1 1 1 0 0 1 1 0 1 1 0 1 1 0 1 0 1 1 0 1 0 0 0\n",
      "  1 0 1 1 0 0 0 0 1 0 0 1 0 1 1 1 1 0 0 0 1 0 0 1 0 1 1 1 0 0 1 0 1 0 0 0\n",
      "  0 0 0 1 0 1 1 0 0 1 0 0 0 0 0 1 1 0 0 1 1 0 1 0 0 0 1 0 1 0 1 0 1 0 1 0\n",
      "  0 1 0 0 0 0 1 0 1 1 1 0 1 1 0 1 1 0 1 1 0 1 1 1 1 0 0 0 0 0 1 0 0 0 0 1\n",
      "  1 0 1 0]\n",
      " [1 0 1 1 0 1 1 1 1 0 1 0 1 1 0 0 1 1 1 1 1 0 1 1 1 1 1 0 0 0 0 1 0 1 0 1\n",
      "  0 0 0 0 0 1 1 0 1 1 0 1 0 1 1 1 0 1 0 0 0 0 0 1 1 1 1 1 0 0 1 0 0 0 0 1\n",
      "  0 1 1 1 0 0 1 1 0 0 1 1 0 0 1 0 0 0 1 0 0 1 0 1 0 1 1 1 1 0 0 0 0 0 0 0\n",
      "  0 1 1 0 0 0 0 0 1 1 1 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 1 0 0 0\n",
      "  0 1 1 0 1 1 0 1 1 0 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1\n",
      "  1 1 0 1 0 0 0 1 0 1 0 1 0 0 1 1 0 1 1 1 0 0 0 0 1 0 1 0 1 1 1 1 0 0 1 0\n",
      "  1 1 0 0 0 0 0 0 1 1 1 1 0 0 1 0 1 1 0 0 1 0 1 0 1 0 0 0 1 0 1 1 0 1 0 1\n",
      "  1 1 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 1 0 0 0 0 0 0\n",
      "  1 0 0 1 0 0 1 0 1 0 1 0 0 0 1 0 0 1 1 1 0 0 1 0 1 0 1 0 1 0 1 1 0 0 0 1\n",
      "  0 1 1 0 0 0 1 1 0 0 0 0 1 0 1 0 0 1 0 1 1 0 1 0 1 1 1 1 0 0 0 1 1 1 0 1\n",
      "  1 0 1 0 0 1 1 1 1 0 0 0 0 1 1 1 1 0 1 1 0 1 1 0 0 0 0 1 1 0 0 1 0 1 1 1\n",
      "  0 1 1 0]]\n"
     ]
    }
   ],
   "source": [
    " X,Y = create_Xor()\n",
    " print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "  \"\"\"\n",
    "      Function that implement sigmoid\n",
    "      :param Z: input value\n",
    "      :type Z: float, array\n",
    "      :return s: sigmoid result of Z \n",
    "      :rtype s:  float or array\n",
    "  \"\"\"\n",
    "  \n",
    "  s = 1/(1+(np.exp(-Z)))\n",
    "  # sigmoid(0) == 0.5\n",
    "  \n",
    "  assert(0.5 == 1/(1+(np.exp(0))))\n",
    "  \n",
    "  return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8807970779778823"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test sigmoid\n",
    "\n",
    "sigmoid(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5       , 0.88079708])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sigmoid d'un array\n",
    "sigmoid_array= sigmoid(np.array([0,2]))\n",
    "sigmoid_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "  \"\"\"\n",
    "      Function that implement relu\n",
    "      :param Z: input value\n",
    "      :type Z: float, array\n",
    "      :return: relu result of Z \n",
    "      :rtype: float or array\n",
    "      \n",
    "  \"\"\"\n",
    "  \n",
    "  # Z>0 c'est un test si Z>0 alors on va avoir 1 sinon on va avoir 0\n",
    "  \n",
    "  r = Z * (Z>0)\n",
    "    \n",
    "  #r = np.maximum(0,Z)\n",
    "   \n",
    "  assert(0.5 == 0.5 * (0.5>0))\n",
    "  assert(0   == -1  * (-1>0))\n",
    "  \n",
    "  \n",
    "  return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivative_sigmoid(dA,activation_cache):\n",
    "    \"\"\"\n",
    "      Function that implement the derivative of sigmoid\n",
    "      :param Z: input value\n",
    "      :type Z: float, array\n",
    "      :return: derivative result of Z \n",
    "      :rtype: float or array\n",
    "    \"\"\"\n",
    "    Z = activation_cache\n",
    "#     print(\"Z.shape = \", Z.shape, \"dA.shape = \", dA.shape)\n",
    "    s = 1/(1+np.exp(-Z))\n",
    "    dZ= dA * s *(1-s)\n",
    "#     print(\"dZ.shape = \", dZ.shape, \"s.shape = \", s.shape)\n",
    "    assert(0.25 == sigmoid (0) * (1-sigmoid(0)))\n",
    "  \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dZ = derivative_sigmoid(45)\n",
    "# dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def derivative_relu(dA,activation_cache):\n",
    "#   \"\"\"\n",
    "#       Function that implement the derivative of relu\n",
    "#       :param Z: input value\n",
    "#       :type Z: float, array\n",
    "#       :return: derivative result of Z \n",
    "#       :rtype: float or array\n",
    "      \n",
    "#   \"\"\"\n",
    "  \n",
    "#   dZ= 1 * (Z>0)\n",
    "  \n",
    "#   assert (0 == 1 * (-1>0) )\n",
    "#   assert (1 == 1 * (1>0) )\n",
    "  \n",
    "#   return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivative_relu(dA, activation_cache):\n",
    "    \n",
    "    Z = activation_cache\n",
    "    dZ = np.array(dA,copy = True)\n",
    "    dZ[Z<= 0] = 0\n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dZ= derivative_relu(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialisation_deep(dim_layers):\n",
    "  \n",
    "    \"\"\" \n",
    "      Function that initialize weights and biais for each layer\n",
    "      :param dim_layers: list of each layer\n",
    "      :type dim_layers: pyhon list\n",
    "      :return parameters dictionnary with W1,b1,......WL,bL\n",
    "      :rtype: python dictionnary\n",
    "      Wl --- weight matrix of shape (dim_layers[l],dim_layers[1-l])\n",
    "      b1 --- weight matrix of shape (dim_layer[l],dim_layers[1-l])\n",
    "    \"\"\"\n",
    "    np.random.seed(3)\n",
    "    parameters={}\n",
    "    \n",
    "    L = len(dim_layers)\n",
    "    \n",
    "    for l in range(1,L):\n",
    "   \n",
    "        parameters[f'W{l}']= np.random.randn(dim_layers[l], dim_layers[l-1])*0.01\n",
    "\n",
    "        parameters[f'b{l}'] = np.zeros((dim_layers[l],1))\n",
    "    \n",
    "#     print(f'W{l}.shape = {dim_layers[l]},{dim_layers[l-1]}')\n",
    "#     print(f\"b{l}.shape = {dim_layers[l]}, 1 \")\n",
    "\n",
    "    assert (parameters[f\"W{l}\"].shape == (dim_layers[l],dim_layers[l-1]))\n",
    "    assert (parameters[f\"b{l}\"].shape == (dim_layers[l],1))\n",
    "    \n",
    "    return parameters\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'W1': array([[ 0.01788628,  0.0043651 ],\n",
       "        [ 0.00096497, -0.01863493],\n",
       "        [-0.00277388, -0.00354759]]), 'b1': array([[0.],\n",
       "        [0.],\n",
       "        [0.]]), 'W2': array([[-0.00082741, -0.00627001, -0.00043818],\n",
       "        [-0.00477218, -0.01313865,  0.00884622],\n",
       "        [ 0.00881318,  0.01709573,  0.00050034],\n",
       "        [-0.00404677, -0.0054536 , -0.01546477]]), 'b2': array([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]]), 'W3': array([[ 0.00982367, -0.01101068, -0.01185047, -0.0020565 ]]), 'b3': array([[0.]])}"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim_layers= [2,3,4,1]\n",
    "parameters = initialisation_deep(dim_layers)\n",
    "\n",
    "parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_forward (A_prev ,W , b, activation):\n",
    "    \"\"\"\n",
    "      function that both computes preactivation and activation \n",
    "      :param A_prev: previews activation matrix (for the first layer it is X )\n",
    "      :param W: weight matrix for the current layer\n",
    "      :param b: biais matrix for the current layer\n",
    "      :param activation: choice of activation function eg: sigmoid , relu\n",
    "      :type A_prev : matrix of float\n",
    "      :type W: matrix of float\n",
    "      :type b: matrix of float\n",
    "      :return: A matrix of activation\n",
    "      :cache: tuple of (linear_cache,activation_cache) \n",
    "      \"\"\"\n",
    "\n",
    "    Z = np.dot(W,A_prev)+b\n",
    "\n",
    "    linear_cache = (A_prev,W,b)\n",
    "  \n",
    "  \n",
    "    assert(Z.shape == (W.shape[0], A_prev.shape[1]))\n",
    "  \n",
    "    if activation == \"sigmoid\":\n",
    "         \n",
    "        A = sigmoid(Z)\n",
    "        activation_cache = A\n",
    "    \n",
    "    elif activation == \"relu\":\n",
    "         \n",
    "        A = relu(Z)\n",
    "        activation_cache = A\n",
    "         \n",
    "         \n",
    "    assert (A.shape == Z.shape)\n",
    "         \n",
    "    cache = (linear_cache, activation_cache)\n",
    "  \n",
    "    return A,cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A,cache = linear_activation_forward(X,parameters['W1'],parameters['b1'],\"relu\")\n",
    "# print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # A_test= np.array([\n",
    "# #            [3,4,5],\n",
    "# #            [3,4,7],\n",
    "#            [4,2,1]\n",
    "#                   ])\n",
    "# # W_test= np.array([\n",
    "#                 [5,3,1],\n",
    "#                 [6,7,9],\n",
    "#                 [3,5,2]\n",
    "#                        ])\n",
    "# # b_test= ([\n",
    "#           [0],\n",
    "#           [0],\n",
    "#           [0]\n",
    "#              ])\n",
    "# # A,cache = linear_activation_forward(A_test,W_test,b_test,\"relu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "# faire une fonction forward sur l layers\n",
    "def forward_layers(X,parameters):\n",
    "    \"\"\"\n",
    "       Function that computes the forward activation for L layers\n",
    "      :param X: input matrix, shape (input_size, number of exemple)\n",
    "      :param parameters: output of initialisation_deep dictionnary of W, b\n",
    "      :type X: matrix of float\n",
    "      :type parameters : dictionary of matrices\n",
    "      :return AL : last post activation value\n",
    "      :return caches : list of caches with every caches of linear_activation_forward \n",
    "      :rtype AL: matrix of float\n",
    "      :rtype caches: list of tuples\n",
    "\n",
    "  \"\"\"\n",
    "    #je crée une  liste de  caches où je vais stoker les valeurs obtenues\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2 # je calcul le nombre de couches par rapport aux nombres de paramètres\n",
    "    #je fais une boucle pour toutes les couches jusqu'à L-1\n",
    "    for l in range (1,L): \n",
    "    #je considère X comme \n",
    "        A_prev= A\n",
    "#       je fais mon calcul pour A0 jusqu'à L-1 ou bien (1 à L)\n",
    "        A,cache = linear_activation_forward(A_prev,parameters[f'W{l}'],parameters[f'b{l}'],\"sigmoid\")\n",
    "       \n",
    "        #je rajoute le cache obtenu dans la liste cache\n",
    "        caches.append(cache)\n",
    "      \n",
    "      #calcul pour la dernière couche:\n",
    "      #je récupère le dernier A qui est sorti de mes couches précédentes et je lui mets une sigmoid \n",
    "      \n",
    "    AL,cache = linear_activation_forward(A,parameters[f'W{l+1}'],parameters[f'b{l+1}'],\"sigmoid\")\n",
    "    caches.append(cache)\n",
    "   \n",
    "    assert AL.shape == (1,X.shape[1])\n",
    "    \n",
    "    return AL,caches\n",
    "\n",
    " \n",
    "  \n",
    "  \n",
    "      \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AL,caches = forward_layers(X,parameters)\n",
    "# AL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #test sur les petits array\n",
    "\n",
    "# parameters_test={\"W1\":W_test,\n",
    "#                  \"b1\":b_test}\n",
    "\n",
    "# ALtest,caches_test = forward_layers(A_test,parameters_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = X.shape[1]\n",
    "\n",
    "def compute_cost(AL,Y):\n",
    "    \"\"\"\n",
    "        Function that compute the cost \n",
    "        :param AL: probability vector - shape (1,number of examples)\n",
    "        :param Y:  matrix of float\n",
    "        :type AL: matrix\n",
    "        :type Y: array of booleen\n",
    "        :return cost: cost result\n",
    "        :rtype: float \n",
    "    \"\"\"\n",
    "    #je calcule d'abord llog\n",
    "    logprob = (Y * np.log(AL) + (1-Y) * np.log(1-AL))\n",
    "    #ensuite la cost\n",
    "    cost = -(np.sum(logprob))/m\n",
    "    #je veux que l'on me retourne un nombre et non pas un array\n",
    "    cost = np.squeeze(cost)\n",
    "    #être sur que j'ai la cost au bon format\n",
    "    assert(isinstance(cost,float))\n",
    "  \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cost = compute_cost(AL,Y)\n",
    "# print(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "#backward L layer\n",
    "def linear_backward(dZ,cache):\n",
    "    \"\"\"\n",
    "        function that computes the linear backward\n",
    "        :param dZ: gradient of the cost with respect to linear output\n",
    "        :param cache: tuple of value\n",
    "        :return dA_prev: gradient of the cost with respect to activation\n",
    "        :return dW: gradient of the cost with respect to W\n",
    "        :return db: gradient of the cost with respect to b\n",
    "    \"\"\"\n",
    "    #recuperation des valeurs dont j'ai besoin\n",
    "    \n",
    "    A_prev, W, b = cache\n",
    "    m=A_prev.shape[1]\n",
    "    #calcul des dérivées:\n",
    "  \n",
    "    dW = np.dot(dZ,A_prev.T)/m\n",
    "    db = np.sum(dZ,axis=1,keepdims = True)/m\n",
    "    dA_prev= np.dot(W.T,dZ)\n",
    "    \n",
    "    assert(dA_prev.shape == A_prev.shape)\n",
    "    assert(dW.shape == W.shape)\n",
    "    assert(db.shape == b.shape)\n",
    "  \n",
    "    return dA_prev,dW,db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fonction backward pour l'implémentation de la backward si fonction d'activation est une sigmoid ou fonction d'activation est une RELU\n",
    "\n",
    "def linear_activation_backward(dA,cache,activation):\n",
    "    \"\"\"  \n",
    "        function that computes the linear activation backward with sigmoid and relu\n",
    "        :param dA: gradient of the activation for the current layer the l layer\n",
    "        :param cache: tuple of values with the parameters\n",
    "        :param activation: activation function-Relu or Sigmoid\n",
    "        :type dA: numpy array\n",
    "        :type activation: string\n",
    "        :return dA_prev: gradient activation of the l-1 layer-shape = A_prev shape\n",
    "        :return dW: gradient of the cost with respect of the W for the current layer l\n",
    "        :return db: gradient of the cost with respect of the b for the current layer l\n",
    "    \"\"\"\n",
    "    #je récupère de mon cache les paramètres \n",
    "\n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    # je calcule dZ en fonction de l'activation RELU ou Sigmoid\n",
    "    if activation == 'relu':\n",
    "        dZ = derivative_relu(dA, activation_cache)\n",
    "        dA_prev,dW,db = linear_backward (dZ,linear_cache)\n",
    "        \n",
    "    elif activation == 'sigmoid':\n",
    "        dZ = derivative_sigmoid(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ,linear_cache)\n",
    "    \n",
    "    return dA_prev, dW, db\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l_model_backward(AL,Y,caches):\n",
    "    \n",
    "    \"\"\" \n",
    "          function that compute the backward propagation\n",
    "          :param AL: array with the last activation -probability vector \n",
    "          :param Y: array with the label\n",
    "          :param caches: list of caches of all the parameters of relu activation and one cache with all the parameters with sigmoid\n",
    "          :type AL: numpy array\n",
    "          :type Y: vectord\n",
    "          :type caches: python list\n",
    "          :return grads: dictionnary of gradients dW,db,dA\n",
    "          \n",
    "    \"\"\"\n",
    "    grads = {}\n",
    "#     print(\"initialisation de grad\")\n",
    "    L = len(caches) #nombre de couches (correspond aux nombres de caches)\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape) #on reshape Y comme AL pour pouvoir faire les opérations\n",
    "#     print(\"L = \", L, \" m = \",m, \" Y = \",Y)\n",
    "    #     initialisation de la back propagation pour calculer dAL\n",
    "   \n",
    "    \n",
    "    dAL = -(np.divide(Y,AL)-np.divide(1-Y,1-AL))\n",
    "#     print(\"dAL=\", dAL)\n",
    "    \n",
    "  \n",
    "      #calcul des gradients pour la dernière couche L sigmoid \n",
    "    #j'utilise le cache de la dernière couche et je mets tout dans un dictionnaire\n",
    "\n",
    "    current_cache = caches[L-1]\n",
    "#     print(\"current_cache = \", current_cache)\n",
    "    grads[f\"dA{L-1}\"], grads[f\"dW{L}\"], grads[f\"db{L}\"] = linear_activation_backward(dAL,current_cache, activation =\"sigmoid\")\n",
    "#     print([f\"dA{L-1}\"], grads[f\"dW{L}\"], grads[f\"db{L}\"])\n",
    "  \n",
    "    #ensuite je fais une boucle pour les autres couches de l= l-2 à l = 0\n",
    "  \n",
    "    for l in reversed(range(L-1)):\n",
    "        \"\"\" \n",
    "#             entrée : la dérivée dA l+1 et le cache de la couche current\n",
    "#             sortie : la dérivée dA l et dWl+1 et dbl+1\n",
    "        \n",
    "#         \"\"\"\n",
    "        \n",
    "        current_cache = caches[l]\n",
    "        \n",
    "#         print(\"l = \", l , \"current_cache W =\", current_cache[0][1].shape)\n",
    "#         #je crée des variables temporaires: dA_prev_temp, dW_temp, db_temp\n",
    "    \n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward (grads[f\"dA{l+1}\"], current_cache, activation = \"sigmoid\")\n",
    "    \n",
    "# #         #je les mets dans le dictionnaire\n",
    "    \n",
    "        grads[f\"dA{l}\"] = dA_prev_temp\n",
    "        grads[f\"dW{l+1}\"] = dW_temp\n",
    "        grads[f\"db{l+1}\"] = db_temp\n",
    "\n",
    "#         print (\"da\", l, grads[f\"dA{l}\"].shape)\n",
    "\n",
    "        \n",
    "    return grads\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters,grads,learning_rate):\n",
    "    \"\"\"\n",
    "        function that update parameters using the gradient descent\n",
    "        :argument parameters: python dictionary with parameters\n",
    "        :argument grads: python dictionnary with all the gradient\n",
    "        :return parameters: python dictionnar with the updated parameters\n",
    "    \"\"\"\n",
    "    L = len (parameters)//2\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * grads[\"dW\" + str(l+1)]\n",
    "            \n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * grads[\"db\" + str(l + 1)]\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fonction globale qui compute tout (on commence pour faire la forward)\n",
    "\n",
    "def L_layer_model(X,Y, dim_layers, learning_rate = 0.0075,num_iterations = 3000, print_cost = False):\n",
    "    \n",
    "    \"\"\"\n",
    "        :param X: matrix of inputs\n",
    "        :param Y: vector of label\n",
    "        :param layers_dims: list that contains the input size and each layer size\n",
    "        :param learningrate: learning rate for the gradient descent\n",
    "        :param num_iterations: number of iterations of the loop\n",
    "        :param print_cost: decide if it print the cost (True)or not (False) \n",
    "        :type X: numpy matrix\n",
    "        :type Y: numpy array\n",
    "        :type layers_dims: python list\n",
    "        :type learningrate float\n",
    "        :type num_iteration: int\n",
    "        :type printcost : bool\n",
    "        :return \n",
    "    \"\"\"\n",
    "    np.random.seed(1)\n",
    "    cost = []\n",
    "    \n",
    "    #initialisation des paramètres\n",
    "    \n",
    "    parameters = initialisation_deep(dim_layers)\n",
    "    \n",
    "    #boucle de 0 à nombre d'iterations:\n",
    "    \n",
    "    for i in range (0,num_iterations):\n",
    "        \n",
    "        #forward propagation l layers\n",
    "        AL,caches = forward_layers(X,parameters)\n",
    "        \n",
    "        #compute cost\n",
    "        cost = compute_cost(AL,Y)\n",
    "        print(cost)\n",
    "        \n",
    "        #L model backward\n",
    "        grads = l_model_backward(AL,Y,caches)\n",
    "        \n",
    "        #update parameters\n",
    "        parameters = update_parameters(parameters,grads,learning_rate)\n",
    "        \n",
    "        # Print the cost every 100 training example\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "        if print_cost and i % 100 == 0:\n",
    "             costs.append(cost)\n",
    "                \n",
    "        # plot the cost\n",
    "    plt.plot(np.squeeze(cost))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per hundreds)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "\n",
    "    return parameters\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6932301206705279\n",
      "0.6932281303051906\n",
      "0.6932261539627754\n",
      "0.693224191544237\n",
      "0.6932222429512316\n",
      "0.6932203080861137\n",
      "0.69321838685193\n",
      "0.6932164791524138\n",
      "0.6932145848919827\n",
      "0.6932127039757302\n",
      "0.6932108363094238\n",
      "0.6932089817994985\n",
      "0.6932071403530528\n",
      "0.6932053118778438\n",
      "0.6932034962822826\n",
      "0.6932016934754287\n",
      "0.6931999033669873\n",
      "0.6931981258673031\n",
      "0.6931963608873555\n",
      "0.6931946083387559\n",
      "0.6931928681337416\n",
      "0.6931911401851714\n",
      "0.6931894244065223\n",
      "0.693187720711884\n",
      "0.6931860290159545\n",
      "0.6931843492340374\n",
      "0.693182681282035\n",
      "0.6931810250764461\n",
      "0.6931793805343606\n",
      "0.6931777475734562\n",
      "0.6931761261119936\n",
      "0.6931745160688123\n",
      "0.6931729173633268\n",
      "0.6931713299155223\n",
      "0.6931697536459503\n",
      "0.6931681884757261\n",
      "0.6931666343265226\n",
      "0.6931650911205677\n",
      "0.6931635587806398\n",
      "0.6931620372300649\n",
      "0.6931605263927102\n",
      "0.6931590261929841\n",
      "0.6931575365558288\n",
      "0.6931560574067183\n",
      "0.693154588671654\n",
      "0.6931531302771613\n",
      "0.6931516821502859\n",
      "0.6931502442185901\n",
      "0.6931488164101477\n",
      "0.6931473986535431\n",
      "0.6931459908778654\n",
      "0.6931445930127065\n",
      "0.6931432049881545\n",
      "0.6931418267347946\n",
      "0.6931404581837021\n",
      "0.6931390992664401\n",
      "0.6931377499150561\n",
      "0.6931364100620786\n",
      "0.6931350796405132\n",
      "0.6931337585838397\n",
      "0.6931324468260087\n",
      "0.6931311443014379\n",
      "0.6931298509450088\n",
      "0.6931285666920636\n",
      "0.6931272914784024\n",
      "0.6931260252402792\n",
      "0.6931247679143986\n",
      "0.6931235194379135\n",
      "0.6931222797484209\n",
      "0.693121048783959\n",
      "0.693119826483005\n",
      "0.6931186127844703\n",
      "0.693117407627699\n",
      "0.6931162109524641\n",
      "0.693115022698964\n",
      "0.6931138428078207\n",
      "0.6931126712200757\n",
      "0.693111507877187\n",
      "0.6931103527210274\n",
      "0.6931092056938801\n",
      "0.6931080667384367\n",
      "0.6931069357977938\n",
      "0.69310581281545\n",
      "0.6931046977353037\n",
      "0.6931035905016506\n",
      "0.6931024910591788\n",
      "0.693101399352968\n",
      "0.6931003153284864\n",
      "0.6930992389315874\n",
      "0.693098170108507\n",
      "0.6930971088058618\n",
      "0.693096054970645\n",
      "0.6930950085502247\n",
      "0.6930939694923408\n",
      "0.693092937745103\n",
      "0.6930919132569875\n",
      "0.6930908959768339\n",
      "0.6930898858538448\n",
      "0.6930888828375802\n",
      "0.6930878868779576\n",
      "0.6930868979252484\n",
      "0.6930859159300736\n",
      "0.6930849408434057\n",
      "0.6930839726165621\n",
      "0.6930830112012039\n",
      "0.6930820565493346\n",
      "0.6930811086132962\n",
      "0.6930801673457679\n",
      "0.6930792326997625\n",
      "0.6930783046286249\n",
      "0.6930773830860297\n",
      "0.6930764680259793\n",
      "0.6930755594027996\n",
      "0.693074657171141\n",
      "0.6930737612859721\n",
      "0.6930728717025813\n",
      "0.6930719883765716\n",
      "0.6930711112638602\n",
      "0.6930702403206754\n",
      "0.6930693755035549\n",
      "0.6930685167693426\n",
      "0.6930676640751879\n",
      "0.6930668173785424\n",
      "0.6930659766371586\n",
      "0.6930651418090867\n",
      "0.6930643128526733\n",
      "0.6930634897265594\n",
      "0.6930626723896777\n",
      "0.6930618608012505\n",
      "0.6930610549207893\n",
      "0.6930602547080897\n",
      "0.6930594601232327\n",
      "0.6930586711265799\n",
      "0.6930578876787736\n",
      "0.6930571097407332\n",
      "0.6930563372736545\n",
      "0.6930555702390072\n",
      "0.6930548085985326\n",
      "0.6930540523142424\n",
      "0.6930533013484163\n",
      "0.6930525556636\n",
      "0.693051815222604\n",
      "0.6930510799885008\n",
      "0.6930503499246242\n",
      "0.6930496249945662\n",
      "0.6930489051621755\n",
      "0.6930481903915566\n",
      "0.6930474806470671\n",
      "0.6930467758933156\n",
      "0.6930460760951612\n",
      "0.6930453812177106\n",
      "0.6930446912263167\n",
      "0.693044006086577\n",
      "0.6930433257643313\n",
      "0.6930426502256608\n",
      "0.6930419794368867\n",
      "0.6930413133645663\n",
      "0.6930406519754939\n",
      "0.693039995236698\n",
      "0.693039343115439\n",
      "0.6930386955792094\n",
      "0.6930380525957298\n",
      "0.6930374141329494\n",
      "0.6930367801590437\n",
      "0.6930361506424114\n",
      "0.6930355255516751\n",
      "0.6930349048556789\n",
      "0.6930342885234859\n",
      "0.6930336765243783\n",
      "0.6930330688278535\n",
      "0.6930324654036261\n",
      "0.6930318662216224\n",
      "0.6930312712519822\n",
      "0.6930306804650548\n",
      "0.6930300938313997\n",
      "0.6930295113217824\n",
      "0.6930289329071768\n",
      "0.6930283585587595\n",
      "0.6930277882479113\n",
      "0.6930272219462148\n",
      "0.693026659625453\n",
      "0.6930261012576071\n",
      "0.6930255468148571\n",
      "0.6930249962695783\n",
      "0.6930244495943408\n",
      "0.6930239067619084\n",
      "0.6930233677452374\n",
      "0.6930228325174737\n",
      "0.6930223010519535\n",
      "0.6930217733221995\n",
      "0.6930212493019233\n",
      "0.6930207289650201\n",
      "0.6930202122855698\n",
      "0.6930196992378348\n",
      "0.6930191897962592\n",
      "0.693018683935467\n",
      "0.6930181816302613\n",
      "0.6930176828556222\n",
      "0.6930171875867072\n",
      "0.6930166957988482\n",
      "0.6930162074675513\n",
      "0.6930157225684946\n",
      "0.693015241077529\n",
      "0.6930147629706737\n",
      "0.6930142882241185\n",
      "0.6930138168142205\n",
      "0.693013348717503\n",
      "0.6930128839106557\n",
      "0.6930124223705317\n",
      "0.6930119640741474\n",
      "0.693011508998682\n",
      "0.693011057121474\n",
      "0.6930106084200232\n",
      "0.6930101628719867\n",
      "0.6930097204551796\n",
      "0.6930092811475735\n",
      "0.693008844927295\n",
      "0.6930084117726247\n",
      "0.6930079816619965\n",
      "0.6930075545739964\n",
      "0.6930071304873602\n",
      "0.6930067093809751\n",
      "0.6930062912338761\n",
      "0.693005876025246\n",
      "0.6930054637344145\n",
      "0.6930050543408568\n",
      "0.6930046478241928\n",
      "0.6930042441641856\n",
      "0.6930038433407415\n",
      "0.693003445333908\n",
      "0.693003050123873\n",
      "0.6930026576909646\n",
      "0.6930022680156483\n",
      "0.6930018810785286\n",
      "0.693001496860346\n",
      "0.693001115341976\n",
      "0.69300073650443\n",
      "0.6930003603288526\n",
      "0.6929999867965209\n",
      "0.6929996158888446\n",
      "0.6929992475873639\n",
      "0.6929988818737488\n",
      "0.692998518729799\n",
      "0.692998158137442\n",
      "0.6929978000787326\n",
      "0.6929974445358519\n",
      "0.6929970914911067\n",
      "0.692996740926929\n",
      "0.6929963928258727\n",
      "0.6929960471706167\n",
      "0.6929957039439608\n",
      "0.6929953631288257\n",
      "0.692995024708253\n",
      "0.6929946886654035\n",
      "0.6929943549835564\n",
      "0.692994023646109\n",
      "0.692993694636575\n",
      "0.6929933679385849\n",
      "0.6929930435358839\n",
      "0.6929927214123316\n",
      "0.6929924015519014\n",
      "0.6929920839386804\n",
      "0.6929917685568663\n",
      "0.6929914553907688\n",
      "0.692991144424808\n",
      "0.692990835643514\n",
      "0.6929905290315256\n",
      "0.6929902245735892\n",
      "0.6929899222545595\n",
      "0.6929896220593968\n",
      "0.6929893239731689\n",
      "0.6929890279810469\n",
      "0.6929887340683075\n",
      "0.6929884422203307\n",
      "0.692988152422599\n",
      "0.6929878646606977\n",
      "0.6929875789203137\n",
      "0.6929872951872338\n",
      "0.6929870134473457\n",
      "0.6929867336866363\n",
      "0.6929864558911905\n",
      "0.692986180047192\n",
      "0.6929859061409211\n",
      "0.6929856341587555\n",
      "0.6929853640871676\n",
      "0.692985095912726\n",
      "0.6929848296220933\n",
      "0.6929845652020264\n",
      "0.6929843026393752\n",
      "0.6929840419210821\n",
      "0.6929837830341813\n",
      "0.6929835259657986\n",
      "0.6929832707031502\n",
      "0.6929830172335423\n",
      "0.6929827655443703\n",
      "0.6929825156231186\n",
      "0.6929822674573595\n",
      "0.6929820210347529\n",
      "0.692981776343045\n",
      "0.6929815333700694\n",
      "0.6929812921037438\n",
      "0.6929810525320725\n",
      "0.6929808146431428\n",
      "0.6929805784251271\n",
      "0.6929803438662799\n",
      "0.692980110954939\n",
      "0.6929798796795242\n",
      "0.6929796500285367\n",
      "0.6929794219905585\n",
      "0.6929791955542526\n",
      "0.6929789707083603\n",
      "0.692978747441704\n",
      "0.6929785257431834\n",
      "0.6929783056017768\n",
      "0.6929780870065401\n",
      "0.6929778699466058\n",
      "0.6929776544111833\n",
      "0.6929774403895579\n",
      "0.69297722787109\n",
      "0.692977016845215\n",
      "0.692976807301443\n",
      "0.6929765992293571\n",
      "0.6929763926186143\n",
      "0.6929761874589441\n",
      "0.6929759837401487\n",
      "0.6929757814521015\n",
      "0.6929755805847474\n",
      "0.6929753811281022\n",
      "0.6929751830722511\n",
      "0.6929749864073506\n",
      "0.6929747911236248\n",
      "0.6929745972113678\n",
      "0.6929744046609408\n",
      "0.6929742134627744\n",
      "0.6929740236073649\n",
      "0.6929738350852768\n",
      "0.6929736478871397\n",
      "0.6929734620036502\n",
      "0.6929732774255695\n",
      "0.6929730941437245\n",
      "0.6929729121490064\n",
      "0.6929727314323703\n",
      "0.6929725519848347\n",
      "0.692972373797482\n",
      "0.6929721968614564\n",
      "0.6929720211679655\n",
      "0.6929718467082779\n",
      "0.6929716734737238\n",
      "0.6929715014556943\n",
      "0.6929713306456415\n",
      "0.6929711610350773\n",
      "0.6929709926155728\n",
      "0.6929708253787595\n",
      "0.6929706593163266\n",
      "0.6929704944200229\n",
      "0.6929703306816544\n",
      "0.6929701680930844\n",
      "0.692970006646235\n",
      "0.6929698463330835\n",
      "0.6929696871456639\n",
      "0.6929695290760671\n",
      "0.6929693721164389\n",
      "0.6929692162589802\n",
      "0.6929690614959475\n",
      "0.692968907819651\n",
      "0.6929687552224556\n",
      "0.692968603696779\n",
      "0.6929684532350933\n",
      "0.6929683038299228\n",
      "0.6929681554738448\n",
      "0.6929680081594882\n",
      "0.6929678618795344\n",
      "0.6929677166267161\n",
      "0.6929675723938167\n",
      "0.6929674291736706\n",
      "0.6929672869591629\n",
      "0.6929671457432285\n",
      "0.6929670055188515\n",
      "0.6929668662790662\n",
      "0.6929667280169549\n",
      "0.6929665907256499\n",
      "0.6929664543983302\n",
      "0.6929663190282238\n",
      "0.6929661846086064\n",
      "0.6929660511327999\n",
      "0.6929659185941744\n",
      "0.6929657869861461\n",
      "0.6929656563021774\n",
      "0.6929655265357766\n",
      "0.6929653976804979\n",
      "0.6929652697299404\n",
      "0.6929651426777491\n",
      "0.6929650165176128\n",
      "0.6929648912432647\n",
      "0.6929647668484827\n",
      "0.6929646433270881\n",
      "0.6929645206729451\n",
      "0.6929643988799622\n",
      "0.6929642779420894\n",
      "0.6929641578533201\n",
      "0.6929640386076903\n",
      "0.6929639201992766\n",
      "0.692963802622198\n",
      "0.6929636858706155\n",
      "0.6929635699387299\n",
      "0.6929634548207835\n",
      "0.692963340511059\n",
      "0.6929632270038795\n",
      "0.6929631142936068\n",
      "0.6929630023746443\n",
      "0.6929628912414328\n",
      "0.6929627808884535\n",
      "0.6929626713102263\n",
      "0.6929625625013088\n",
      "0.6929624544562973\n",
      "0.6929623471698266\n",
      "0.6929622406365684\n",
      "0.6929621348512321\n",
      "0.6929620298085646\n",
      "0.6929619255033495\n",
      "0.6929618219304071\n",
      "0.6929617190845937\n",
      "0.6929616169608025\n",
      "0.6929615155539619\n",
      "0.6929614148590371\n",
      "0.692961314871027\n",
      "0.6929612155849668\n",
      "0.6929611169959261\n",
      "0.6929610190990099\n",
      "0.6929609218893564\n",
      "0.6929608253621393\n",
      "0.692960729512565\n",
      "0.6929606343358745\n",
      "0.6929605398273415\n",
      "0.692960445982274\n",
      "0.6929603527960114\n",
      "0.6929602602639274\n",
      "0.6929601683814275\n",
      "0.6929600771439494\n",
      "0.6929599865469629\n",
      "0.6929598965859698\n",
      "0.6929598072565036\n",
      "0.6929597185541292\n",
      "0.6929596304744419\n",
      "0.6929595430130694\n",
      "0.6929594561656687\n",
      "0.6929593699279281\n",
      "0.6929592842955661\n",
      "0.6929591992643311\n",
      "0.6929591148300014\n",
      "0.6929590309883852\n",
      "0.6929589477353201\n",
      "0.6929588650666729\n",
      "0.6929587829783389\n",
      "0.6929587014662433\n",
      "0.692958620526339\n",
      "0.6929585401546083\n",
      "0.6929584603470604\n",
      "0.6929583810997341\n",
      "0.6929583024086946\n",
      "0.6929582242700356\n",
      "0.692958146679878\n",
      "0.6929580696343701\n",
      "0.6929579931296872\n",
      "0.6929579171620313\n",
      "0.692957841727631\n",
      "0.6929577668227421\n",
      "0.6929576924436458\n",
      "0.6929576185866503\n",
      "0.692957545248089\n",
      "0.6929574724243212\n",
      "0.6929574001117323\n",
      "0.6929573283067326\n",
      "0.6929572570057577\n",
      "0.6929571862052682\n",
      "0.6929571159017501\n",
      "0.6929570460917134\n",
      "0.6929569767716929\n",
      "0.6929569079382476\n",
      "0.6929568395879611\n",
      "0.6929567717174402\n",
      "0.6929567043233166\n",
      "0.692956637402245\n",
      "0.6929565709509036\n",
      "0.692956504965994\n",
      "0.6929564394442412\n",
      "0.6929563743823931\n",
      "0.6929563097772202\n",
      "0.692956245625516\n",
      "0.6929561819240962\n",
      "0.6929561186697992\n",
      "0.6929560558594858\n",
      "0.6929559934900379\n",
      "0.6929559315583608\n",
      "0.6929558700613796\n",
      "0.6929558089960431\n",
      "0.6929557483593203\n",
      "0.6929556881482013\n",
      "0.692955628359698\n",
      "0.6929555689908432\n",
      "0.6929555100386903\n",
      "0.6929554515003136\n",
      "0.6929553933728075\n",
      "0.6929553356532877\n",
      "0.6929552783388893\n",
      "0.6929552214267679\n",
      "0.692955164914099\n",
      "0.6929551087980781\n",
      "0.6929550530759204\n",
      "0.6929549977448601\n",
      "0.692954942802152\n",
      "0.6929548882450686\n",
      "0.692954834070903\n",
      "0.6929547802769666\n",
      "0.6929547268605899\n",
      "0.6929546738191218\n",
      "0.6929546211499301\n",
      "0.6929545688504015\n",
      "0.6929545169179403\n",
      "0.6929544653499693\n",
      "0.6929544141439298\n",
      "0.6929543632972807\n",
      "0.6929543128074985\n",
      "0.692954262672078\n",
      "0.6929542128885311\n",
      "0.6929541634543877\n",
      "0.6929541143671943\n",
      "0.6929540656245158\n",
      "0.6929540172239327\n",
      "0.6929539691630439\n",
      "0.6929539214394639\n",
      "0.6929538740508252\n",
      "0.6929538269947763\n",
      "0.6929537802689816\n",
      "0.6929537338711235\n",
      "0.692953687798899\n",
      "0.6929536420500223\n",
      "0.6929535966222233\n",
      "0.692953551513248\n",
      "0.6929535067208579\n",
      "0.6929534622428306\n",
      "0.6929534180769593\n",
      "0.6929533742210526\n",
      "0.692953330672934\n",
      "0.692953287430443\n",
      "0.6929532444914347\n",
      "0.6929532018537773\n",
      "0.6929531595153564\n",
      "0.6929531174740708\n",
      "0.6929530757278343\n",
      "0.692953034274576\n",
      "0.6929529931122391\n",
      "0.6929529522387814\n",
      "0.6929529116521748\n",
      "0.6929528713504055\n",
      "0.6929528313314742\n",
      "0.6929527915933952\n",
      "0.6929527521341968\n",
      "0.6929527129519218\n",
      "0.6929526740446255\n",
      "0.6929526354103781\n",
      "0.6929525970472628\n",
      "0.6929525589533759\n",
      "0.6929525211268276\n",
      "0.6929524835657415\n",
      "0.6929524462682539\n",
      "0.6929524092325144\n",
      "0.6929523724566857\n",
      "0.6929523359389431\n",
      "0.6929522996774752\n",
      "0.6929522636704826\n",
      "0.6929522279161792\n",
      "0.6929521924127914\n",
      "0.6929521571585576\n",
      "0.692952122151729\n",
      "0.6929520873905686\n",
      "0.6929520528733524\n",
      "0.6929520185983674\n",
      "0.6929519845639136\n",
      "0.6929519507683028\n",
      "0.6929519172098583\n",
      "0.6929518838869149\n",
      "0.6929518507978198\n",
      "0.692951817940932\n",
      "0.6929517853146209\n",
      "0.692951752917268\n",
      "0.6929517207472665\n",
      "0.6929516888030204\n",
      "0.692951657082945\n",
      "0.6929516255854666\n",
      "0.6929515943090233\n",
      "0.6929515632520628\n",
      "0.6929515324130451\n",
      "0.69295150179044\n",
      "0.6929514713827288\n",
      "0.6929514411884028\n",
      "0.6929514112059644\n",
      "0.6929513814339262\n",
      "0.6929513518708117\n",
      "0.6929513225151541\n",
      "0.6929512933654973\n",
      "0.6929512644203953\n",
      "0.6929512356784124\n",
      "0.6929512071381231\n",
      "0.6929511787981112\n",
      "0.6929511506569712\n",
      "0.6929511227133074\n",
      "0.6929510949657336\n",
      "0.6929510674128733\n",
      "0.6929510400533596\n",
      "0.6929510128858363\n",
      "0.6929509859089548\n",
      "0.6929509591213775\n",
      "0.692950932521776\n",
      "0.6929509061088303\n",
      "0.6929508798812307\n",
      "0.692950853837676\n",
      "0.6929508279768746\n",
      "0.6929508022975438\n",
      "0.6929507767984098\n",
      "0.692950751478208\n",
      "0.6929507263356822\n",
      "0.6929507013695856\n",
      "0.6929506765786799\n",
      "0.6929506519617354\n",
      "0.6929506275175313\n",
      "0.6929506032448549\n",
      "0.6929505791425024\n",
      "0.6929505552092783\n",
      "0.692950531443996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6929505078454763\n",
      "0.6929504844125486\n",
      "0.6929504611440513\n",
      "0.69295043803883\n",
      "0.6929504150957385\n",
      "0.6929503923136394\n",
      "0.6929503696914026\n",
      "0.6929503472279057\n",
      "0.692950324922035\n",
      "0.6929503027726842\n",
      "0.6929502807787545\n",
      "0.6929502589391555\n",
      "0.6929502372528037\n",
      "0.6929502157186238\n",
      "0.6929501943355473\n",
      "0.6929501731025144\n",
      "0.6929501520184718\n",
      "0.6929501310823732\n",
      "0.6929501102931814\n",
      "0.6929500896498644\n",
      "0.6929500691513991\n",
      "0.6929500487967684\n",
      "0.692950028584963\n",
      "0.6929500085149806\n",
      "0.6929499885858257\n",
      "0.6929499687965099\n",
      "0.6929499491460525\n",
      "0.6929499296334777\n",
      "0.6929499102578188\n",
      "0.6929498910181144\n",
      "0.6929498719134108\n",
      "0.6929498529427601\n",
      "0.6929498341052218\n",
      "0.6929498153998614\n",
      "0.6929497968257516\n",
      "0.6929497783819711\n",
      "0.6929497600676054\n",
      "0.692949741881746\n",
      "0.6929497238234912\n",
      "0.6929497058919455\n",
      "0.6929496880862195\n",
      "0.6929496704054303\n",
      "0.692949652848701\n",
      "0.6929496354151613\n",
      "0.6929496181039463\n",
      "0.6929496009141977\n",
      "0.6929495838450628\n",
      "0.6929495668956954\n",
      "0.6929495500652549\n",
      "0.692949533352907\n",
      "0.6929495167578225\n",
      "0.6929495002791785\n",
      "0.6929494839161581\n",
      "0.6929494676679496\n",
      "0.6929494515337475\n",
      "0.6929494355127517\n",
      "0.6929494196041678\n",
      "0.6929494038072065\n",
      "0.6929493881210846\n",
      "0.6929493725450249\n",
      "0.6929493570782543\n",
      "0.6929493417200058\n",
      "0.6929493264695185\n",
      "0.6929493113260357\n",
      "0.6929492962888064\n",
      "0.6929492813570852\n",
      "0.6929492665301313\n",
      "0.6929492518072101\n",
      "0.6929492371875909\n",
      "0.6929492226705495\n",
      "0.6929492082553651\n",
      "0.6929491939413236\n",
      "0.692949179727715\n",
      "0.6929491656138345\n",
      "0.6929491515989824\n",
      "0.6929491376824635\n",
      "0.692949123863588\n",
      "0.6929491101416704\n",
      "0.6929490965160309\n",
      "0.6929490829859932\n",
      "0.6929490695508866\n",
      "0.6929490562100449\n",
      "0.692949042962807\n",
      "0.6929490298085156\n",
      "0.6929490167465185\n",
      "0.6929490037761681\n",
      "0.6929489908968216\n",
      "0.69294897810784\n",
      "0.6929489654085892\n",
      "0.6929489527984394\n",
      "0.6929489402767658\n",
      "0.692948927842947\n",
      "0.6929489154963671\n",
      "0.6929489032364131\n",
      "0.6929488910624783\n",
      "0.692948878973958\n",
      "0.692948866970253\n",
      "0.6929488550507685\n",
      "0.6929488432149133\n",
      "0.6929488314621007\n",
      "0.6929488197917477\n",
      "0.6929488082032762\n",
      "0.6929487966961108\n",
      "0.6929487852696815\n",
      "0.6929487739234216\n",
      "0.6929487626567686\n",
      "0.692948751469164\n",
      "0.6929487403600529\n",
      "0.6929487293288846\n",
      "0.6929487183751122\n",
      "0.6929487074981924\n",
      "0.692948696697586\n",
      "0.6929486859727578\n",
      "0.6929486753231757\n",
      "0.692948664748312\n",
      "0.692948654247642\n",
      "0.6929486438206451\n",
      "0.6929486334668047\n",
      "0.6929486231856071\n",
      "0.6929486129765431\n",
      "0.6929486028391061\n",
      "0.6929485927727933\n",
      "0.692948582777106\n",
      "0.6929485728515488\n",
      "0.6929485629956288\n",
      "0.6929485532088582\n",
      "0.6929485434907514\n",
      "0.6929485338408266\n",
      "0.6929485242586055\n",
      "0.6929485147436129\n",
      "0.6929485052953769\n",
      "0.6929484959134293\n",
      "0.692948486597305\n",
      "0.6929484773465423\n",
      "0.692948468160682\n",
      "0.6929484590392693\n",
      "0.6929484499818516\n",
      "0.6929484409879797\n",
      "0.6929484320572081\n",
      "0.6929484231890939\n",
      "0.6929484143831977\n",
      "0.6929484056390822\n",
      "0.6929483969563145\n",
      "0.6929483883344641\n",
      "0.6929483797731038\n",
      "0.6929483712718083\n",
      "0.6929483628301568\n",
      "0.6929483544477307\n",
      "0.6929483461241144\n",
      "0.6929483378588951\n",
      "0.6929483296516633\n",
      "0.6929483215020119\n",
      "0.6929483134095369\n",
      "0.6929483053738374\n",
      "0.6929482973945147\n",
      "0.6929482894711732\n",
      "0.69294828160342\n",
      "0.6929482737908659\n",
      "0.6929482660331224\n",
      "0.6929482583298057\n",
      "0.692948250680534\n",
      "0.6929482430849271\n",
      "0.6929482355426094\n",
      "0.6929482280532069\n",
      "0.6929482206163481\n",
      "0.6929482132316642\n",
      "0.6929482058987895\n",
      "0.6929481986173599\n",
      "0.6929481913870148\n",
      "0.692948184207396\n",
      "0.6929481770781472\n",
      "0.6929481699989151\n",
      "0.6929481629693487\n",
      "0.6929481559890996\n",
      "0.6929481490578218\n",
      "0.6929481421751714\n",
      "0.6929481353408076\n",
      "0.6929481285543915\n",
      "0.6929481218155863\n",
      "0.6929481151240586\n",
      "0.6929481084794762\n",
      "0.6929481018815099\n",
      "0.6929480953298324\n",
      "0.692948088824119\n",
      "0.6929480823640478\n",
      "0.6929480759492975\n",
      "0.6929480695795508\n",
      "0.6929480632544918\n",
      "0.6929480569738067\n",
      "0.6929480507371848\n",
      "0.6929480445443161\n",
      "0.6929480383948942\n",
      "0.6929480322886142\n",
      "0.6929480262251734\n",
      "0.6929480202042708\n",
      "0.6929480142256085\n",
      "0.6929480082888898\n",
      "0.6929480023938203\n",
      "0.6929479965401082\n",
      "0.6929479907274632\n",
      "0.6929479849555968\n",
      "0.6929479792242234\n",
      "0.6929479735330585\n",
      "0.69294796788182\n",
      "0.6929479622702281\n",
      "0.6929479566980044\n",
      "0.6929479511648726\n",
      "0.6929479456705585\n",
      "0.6929479402147899\n",
      "0.6929479347972961\n",
      "0.6929479294178087\n",
      "0.6929479240760612\n",
      "0.6929479187717885\n",
      "0.6929479135047276\n",
      "0.6929479082746179\n",
      "0.6929479030811998\n",
      "0.6929478979242161\n",
      "0.6929478928034106\n",
      "0.6929478877185301\n",
      "0.6929478826693223\n",
      "0.6929478776555368\n",
      "0.6929478726769253\n",
      "0.6929478677332409\n",
      "0.6929478628242388\n",
      "0.6929478579496751\n",
      "0.6929478531093085\n",
      "0.6929478483028991\n",
      "0.6929478435302088\n",
      "0.6929478387910009\n",
      "0.69294783408504\n",
      "0.6929478294120937\n",
      "0.6929478247719296\n",
      "0.6929478201643182\n",
      "0.6929478155890311\n",
      "0.6929478110458411\n",
      "0.6929478065345234\n",
      "0.6929478020548544\n",
      "0.6929477976066118\n",
      "0.692947793189575\n",
      "0.6929477888035254\n",
      "0.6929477844482455\n",
      "0.6929477801235193\n",
      "0.6929477758291326\n",
      "0.6929477715648724\n",
      "0.6929477673305274\n",
      "0.6929477631258878\n",
      "0.6929477589507451\n",
      "0.6929477548048922\n",
      "0.692947750688124\n",
      "0.692947746600236\n",
      "0.6929477425410258\n",
      "0.692947738510292\n",
      "0.6929477345078352\n",
      "0.6929477305334566\n",
      "0.6929477265869596\n",
      "0.6929477226681486\n",
      "0.6929477187768288\n",
      "0.6929477149128078\n",
      "0.6929477110758938\n",
      "0.6929477072658969\n",
      "0.692947703482628\n",
      "0.6929476997259002\n",
      "0.6929476959955261\n",
      "0.6929476922913218\n",
      "0.6929476886131036\n",
      "0.6929476849606888\n",
      "0.6929476813338964\n",
      "0.6929476777325471\n",
      "0.6929476741564617\n",
      "0.6929476706054635\n",
      "0.6929476670793764\n",
      "0.6929476635780256\n",
      "0.6929476601012374\n",
      "0.6929476566488397\n",
      "0.6929476532206613\n",
      "0.692947649816532\n",
      "0.6929476464362836\n",
      "0.6929476430797482\n",
      "0.6929476397467593\n",
      "0.6929476364371522\n",
      "0.6929476331507624\n",
      "0.6929476298874274\n",
      "0.6929476266469851\n",
      "0.6929476234292751\n",
      "0.6929476202341377\n",
      "0.6929476170614152\n",
      "0.6929476139109494\n",
      "0.6929476107825846\n",
      "0.6929476076761659\n",
      "0.6929476045915393\n",
      "0.6929476015285515\n",
      "0.692947598487051\n",
      "0.6929475954668872\n",
      "0.6929475924679102\n",
      "0.6929475894899713\n",
      "0.6929475865329231\n",
      "0.6929475835966188\n",
      "0.6929475806809131\n",
      "0.6929475777856614\n",
      "0.6929475749107203\n",
      "0.692947572055947\n",
      "0.6929475692212003\n",
      "0.6929475664063398\n",
      "0.6929475636112256\n",
      "0.6929475608357194\n",
      "0.6929475580796839\n",
      "0.6929475553429819\n",
      "0.6929475526254779\n",
      "0.6929475499270379\n",
      "0.6929475472475272\n",
      "0.6929475445868135\n",
      "0.6929475419447652\n",
      "0.6929475393212507\n",
      "0.6929475367161405\n",
      "0.692947534129305\n",
      "0.6929475315606166\n",
      "0.6929475290099475\n",
      "0.6929475264771716\n",
      "0.6929475239621631\n",
      "0.6929475214647975\n",
      "0.6929475189849511\n",
      "0.6929475165225005\n",
      "0.6929475140773246\n",
      "0.6929475116493015\n",
      "0.6929475092383107\n",
      "0.6929475068442335\n",
      "0.6929475044669505\n",
      "0.6929475021063441\n",
      "0.6929474997622976\n",
      "0.6929474974346945\n",
      "0.6929474951234197\n",
      "0.6929474928283582\n",
      "0.6929474905493964\n",
      "0.6929474882864218\n",
      "0.6929474860393219\n",
      "0.6929474838079854\n",
      "0.6929474815923016\n",
      "0.6929474793921608\n",
      "0.6929474772074539\n",
      "0.6929474750380726\n",
      "0.6929474728839093\n",
      "0.6929474707448577\n",
      "0.6929474686208114\n",
      "0.6929474665116652\n",
      "0.6929474644173145\n",
      "0.6929474623376554\n",
      "0.692947460272585\n",
      "0.6929474582220013\n",
      "0.6929474561858021\n",
      "0.6929474541638868\n",
      "0.6929474521561548\n",
      "0.6929474501625073\n",
      "0.692947448182845\n",
      "0.6929474462170699\n",
      "0.6929474442650846\n",
      "0.692947442326792\n",
      "0.6929474404020965\n",
      "0.6929474384909027\n",
      "0.6929474365931156\n",
      "0.6929474347086415\n",
      "0.6929474328373867\n",
      "0.6929474309792585\n",
      "0.6929474291341652\n",
      "0.6929474273020146\n",
      "0.6929474254827164\n",
      "0.6929474236761809\n",
      "0.6929474218823174\n",
      "0.692947420101038\n",
      "0.692947418332254\n",
      "0.6929474165758777\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "parameters = L_layer_model(X,Y,dim_layers,learning_rate = 0.0075,num_iterations = 1000, print_cost = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "gist": {
   "data": {
    "description": "neural_l_layerfromscratchV2.ipynb",
    "public": false
   },
   "id": ""
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
