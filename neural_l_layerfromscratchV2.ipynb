{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_Xor(nb_features=400):\n",
    "  \n",
    "    \"\"\"\n",
    "        Function that creates XOR dataset\n",
    "        :param nb_features: number of features\n",
    "        :type nb_features : int\n",
    "        :return: X  input numpy array shape(2,400)\n",
    "        :rtype: numpy array\n",
    "        :return: Y true-false labels numpy array shape(400,1)\n",
    "        :rtype: numpy array\n",
    "    \n",
    "      \"\"\"\n",
    "    X = np.random.randint(2, size = (2,nb_features))\n",
    "    Y = np.logical_xor(X[0,:],X[1,:])\n",
    "    print(Y)\n",
    "  \n",
    "    #reshape Y\n",
    "  \n",
    "    Y = Y.reshape(1,400)\n",
    "                        \n",
    "    print(f\"X.shape = {X.shape}\")\n",
    "    print(f\"Y.shape = {Y.shape}\")\n",
    "                        \n",
    "    return X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ True False False False False  True  True False False False  True False\n",
      " False False  True  True False False False False False  True False  True\n",
      " False  True False  True False False  True  True False  True  True False\n",
      " False  True  True False False  True False  True  True  True False  True\n",
      " False False  True False  True False False False  True False  True False\n",
      " False False  True  True  True  True  True False False  True False  True\n",
      "  True False  True False  True  True False False False False False False\n",
      " False False  True False  True False  True  True  True  True  True False\n",
      "  True False  True  True False  True False False  True  True  True False\n",
      " False False  True  True  True False  True False False  True False False\n",
      "  True False False  True False False  True False  True False False  True\n",
      " False  True False False  True False False False  True False  True  True\n",
      "  True  True  True False False  True  True  True  True  True  True False\n",
      "  True  True False  True False False  True False  True False  True  True\n",
      " False False  True False False False  True False False  True  True False\n",
      "  True False  True  True  True False  True False False  True False False\n",
      "  True  True  True  True  True False False False False False False  True\n",
      " False  True  True False  True False False False False False  True  True\n",
      "  True False False False False False False False False  True  True  True\n",
      " False  True  True False False False  True False False False False False\n",
      " False False  True False False False False  True False False False False\n",
      " False  True False False False False False False  True  True False  True\n",
      "  True False False False False  True False False False  True False False\n",
      "  True  True  True  True  True False  True False False  True False  True\n",
      "  True False  True  True  True  True  True False False  True  True  True\n",
      " False  True  True  True  True  True  True  True False  True  True  True\n",
      " False False  True  True  True  True  True  True False  True  True False\n",
      "  True False  True  True False  True  True  True False False False  True\n",
      " False False False  True  True False False False False False  True False\n",
      " False False  True  True False False  True  True  True  True False False\n",
      "  True False False False False False  True  True False  True  True  True\n",
      "  True False  True False  True False  True  True  True False False  True\n",
      " False False  True False False False  True False  True  True  True  True\n",
      " False  True False  True]\n",
      "X.shape = (2, 400)\n",
      "Y.shape = (1, 400)\n",
      "[[1 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 1 1 0 0 1 1 0 0 1 1 1 1 0\n",
      "  0 1 1 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0 1 1 1 0 0 1 0 1 0 1 1 0 0 1 0 0 1 1\n",
      "  1 1 1 0 0 0 0 0 1 1 1 0 0 0 0 0 0 1 0 1 0 0 0 1 0 1 1 1 0 1 1 0 1 0 0 1\n",
      "  0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1 0 1 0 1 0 1 1 0 1 0 1 0 1 0 1 1\n",
      "  1 1 1 0 1 0 1 1 1 1 0 0 0 1 0 1 0 0 1 0 1 0 0 0 1 0 0 1 1 1 0 1 0 1 0 1\n",
      "  1 0 1 1 0 1 1 0 0 0 1 1 1 0 1 0 0 0 1 0 1 0 0 0 1 0 1 1 1 1 1 0 0 1 0 1\n",
      "  1 0 0 1 0 1 0 0 0 0 1 1 0 1 0 0 0 1 0 0 0 1 1 1 0 0 0 1 0 1 0 0 0 1 0 1\n",
      "  1 1 0 0 0 1 1 0 0 0 0 0 0 1 0 1 0 1 1 1 1 0 0 0 0 1 0 0 1 0 1 0 0 1 0 0\n",
      "  1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 1 1 1 0 1 1 1 1 1 0 0 1 0 0 1 0 1 1 1 1 0\n",
      "  0 1 0 0 1 1 0 1 1 0 0 0 1 0 0 1 0 1 1 1 0 0 1 1 0 0 1 0 1 1 1 0 0 1 1 1\n",
      "  0 0 1 0 0 0 0 0 1 1 0 0 0 1 0 0 0 1 0 1 1 0 0 1 1 1 1 1 1 1 0 1 1 1 0 1\n",
      "  0 0 0 0]\n",
      " [0 0 0 1 1 1 0 0 0 0 1 0 0 0 1 1 1 0 0 0 0 0 1 0 1 1 0 0 1 0 1 0 1 0 0 0\n",
      "  0 0 0 0 0 1 0 1 1 1 0 0 1 0 1 0 0 0 1 1 0 0 1 1 0 1 1 0 0 1 1 1 0 1 1 0\n",
      "  0 1 0 0 1 1 0 0 1 1 1 0 0 0 1 0 1 1 1 0 1 1 1 1 1 1 0 0 0 0 1 0 0 1 1 1\n",
      "  0 0 0 1 1 0 1 0 0 1 0 0 0 0 0 0 0 1 1 1 1 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0\n",
      "  0 0 0 0 1 1 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 1 1 0 1 1 1 1 1 1 0 0 1 1\n",
      "  0 0 0 0 1 1 0 0 0 1 1 1 0 1 0 1 1 0 1 0 1 0 0 1 1 1 0 1 0 1 1 0 0 1 1 0\n",
      "  0 0 0 1 0 1 0 0 0 1 0 0 0 0 1 0 0 1 1 0 0 1 1 1 0 0 1 1 0 1 0 1 0 1 0 1\n",
      "  1 0 0 0 0 1 1 0 1 1 0 1 1 1 0 1 0 0 1 1 1 1 0 0 1 0 1 1 0 0 0 0 0 0 0 1\n",
      "  0 1 0 1 1 0 0 1 1 0 0 1 1 0 1 0 0 0 1 0 1 0 0 0 0 0 0 1 1 0 1 0 1 0 0 0\n",
      "  1 1 1 1 1 0 1 0 1 0 0 1 1 0 0 0 1 1 1 1 0 0 0 1 0 0 0 1 1 1 0 1 1 0 1 1\n",
      "  1 0 1 0 0 0 1 1 1 0 1 1 1 1 1 0 1 1 1 0 0 0 0 0 1 1 0 1 1 1 1 1 0 0 1 0\n",
      "  0 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    " X,Y = create_Xor()\n",
    " print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "  \"\"\"\n",
    "      Function that implement sigmoid\n",
    "      :param Z: input value\n",
    "      :type Z: float, array\n",
    "      :return s: sigmoid result of Z \n",
    "      :rtype s:  float or array\n",
    "  \"\"\"\n",
    "  \n",
    "  s = 1/(1+(np.exp(-Z)))\n",
    "  # sigmoid(0) == 0.5\n",
    "  \n",
    "  assert(0.5 == 1/(1+(np.exp(0))))\n",
    "  \n",
    "  return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8807970779778823"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test sigmoid\n",
    "\n",
    "sigmoid(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5       , 0.88079708])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sigmoid d'un array\n",
    "sigmoid_array= sigmoid(np.array([0,2]))\n",
    "sigmoid_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "  \"\"\"\n",
    "      Function that implement relu\n",
    "      :param Z: input value\n",
    "      :type Z: float, array\n",
    "      :return: relu result of Z \n",
    "      :rtype: float or array\n",
    "      \n",
    "  \"\"\"\n",
    "  \n",
    "  # Z>0 c'est un test si Z>0 alors on va avoir 1 sinon on va avoir 0\n",
    "  \n",
    "  r = Z * (Z>0)\n",
    "    \n",
    "  #r = np.maximum(0,Z)\n",
    "   \n",
    "  assert(0.5 == 0.5 * (0.5>0))\n",
    "  assert(0   == -1  * (-1>0))\n",
    "  \n",
    "  \n",
    "  return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivative_sigmoid(dA,activation_cache):\n",
    "    \"\"\"\n",
    "      Function that implement the derivative of sigmoid\n",
    "      :param Z: input value\n",
    "      :type Z: float, array\n",
    "      :return: derivative result of Z \n",
    "      :rtype: float or array\n",
    "    \"\"\"\n",
    "    Z = activation_cache\n",
    "    s = 1/(1+np.exp(-Z))\n",
    "    dZ= dA * s *(1-s)\n",
    "  \n",
    "    assert(0.25 == sigmoid (0) * (1-sigmoid(0)))\n",
    "  \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dZ = derivative_sigmoid(45)\n",
    "# dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def derivative_relu(dA,activation_cache):\n",
    "#   \"\"\"\n",
    "#       Function that implement the derivative of relu\n",
    "#       :param Z: input value\n",
    "#       :type Z: float, array\n",
    "#       :return: derivative result of Z \n",
    "#       :rtype: float or array\n",
    "      \n",
    "#   \"\"\"\n",
    "  \n",
    "#   dZ= 1 * (Z>0)\n",
    "  \n",
    "#   assert (0 == 1 * (-1>0) )\n",
    "#   assert (1 == 1 * (1>0) )\n",
    "  \n",
    "#   return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivative_relu(dA, activation_cache):\n",
    "    \n",
    "    Z = activation_cache\n",
    "    dZ = np.array(dA,copy = True)\n",
    "    dZ[Z<= 0] = 0\n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dZ= derivative_relu(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialisation_deep(dim_layers):\n",
    "  \n",
    "  \"\"\" \n",
    "      Function that initialize weights and biais for each layer\n",
    "      :param dim_layers: list of each layer\n",
    "      :type dim_layers: pyhon list\n",
    "      :return parameters dictionnary with W1,b1,......WL,bL\n",
    "      :rtype: python dictionnary\n",
    "      Wl --- weight matrix of shape (dim_layers[l],dim_layers[1-l])\n",
    "      b1 --- weight matrix of shape (dim_layer[l],dim_layers[1-l])\n",
    "  \"\"\"\n",
    "  np.random.seed(3)\n",
    "  parameters={}\n",
    "    \n",
    "  L = len(dim_layers)\n",
    "    \n",
    "  for l in range(1,L):\n",
    "   \n",
    "    parameters[f'W{l}']= np.random.randn(dim_layers[l], dim_layers[l-1])*0.01\n",
    "      \n",
    "    parameters[f'b{l}'] = np.zeros((dim_layers[l],1))\n",
    "    \n",
    "    print(f'W{l}.shape = {dim_layers[l]},{dim_layers[l-1]}')\n",
    "    print(f\"b{l}.shape = {dim_layers[l]}, 1 \")\n",
    "\n",
    "    assert (parameters[f\"W{l}\"].shape == (dim_layers[l],dim_layers[l-1]))\n",
    "    assert (parameters[f\"b{l}\"].shape == (dim_layers[l],1))\n",
    "    \n",
    "  return parameters\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1.shape = 3,2\n",
      "b1.shape = 3, 1 \n",
      "W2.shape = 4,3\n",
      "b2.shape = 4, 1 \n",
      "W3.shape = 5,4\n",
      "b3.shape = 5, 1 \n",
      "W4.shape = 1,5\n",
      "b4.shape = 1, 1 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'W1': array([[ 0.01788628,  0.0043651 ],\n",
       "        [ 0.00096497, -0.01863493],\n",
       "        [-0.00277388, -0.00354759]]), 'b1': array([[0.],\n",
       "        [0.],\n",
       "        [0.]]), 'W2': array([[-0.00082741, -0.00627001, -0.00043818],\n",
       "        [-0.00477218, -0.01313865,  0.00884622],\n",
       "        [ 0.00881318,  0.01709573,  0.00050034],\n",
       "        [-0.00404677, -0.0054536 , -0.01546477]]), 'b2': array([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]]), 'W3': array([[ 0.00982367, -0.01101068, -0.01185047, -0.0020565 ],\n",
       "        [ 0.01486148,  0.00236716, -0.01023785, -0.00712993],\n",
       "        [ 0.00625245, -0.00160513, -0.00768836, -0.00230031],\n",
       "        [ 0.00745056,  0.01976111, -0.01244123, -0.00626417],\n",
       "        [-0.00803766, -0.02419083, -0.00923792, -0.01023876]]), 'b3': array([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]]), 'W4': array([[ 0.01123978, -0.00131914, -0.01623285,  0.00646675, -0.00356271]]), 'b4': array([[0.]])}"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim_layers= [2,3,4,5,1]\n",
    "parameters = initialisation_deep(dim_layers)\n",
    "\n",
    "parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_forward (A_prev ,W , b, activation):\n",
    "  \"\"\"\n",
    "      function that both computes preactivation and activation \n",
    "      :param A_prev: previews activation matrix (for the first layer it is X )\n",
    "      :param W: weight matrix for the current layer\n",
    "      :param b: biais matrix for the current layer\n",
    "      :param activation: choice of activation function eg: sigmoid , relu\n",
    "      :type A_prev : matrix of float\n",
    "      :type W: matrix of float\n",
    "      :type b: matrix of float\n",
    "      :return: A matrix of activation\n",
    "      :cache: tuple of (linear_cache,activation_cache) \n",
    "  \"\"\"\n",
    "  Z = np.dot(W,A_prev)+b\n",
    "\n",
    "  linear_cache = (A_prev,W,b)\n",
    "  \n",
    "  \n",
    "  assert(Z.shape == (W.shape[0], A_prev.shape[1]))\n",
    "  \n",
    "  if activation == \"sigmoid\":\n",
    "         \n",
    "    A = sigmoid(Z)\n",
    "    activation_cache = A\n",
    "    \n",
    "  elif activation == \"relu\":\n",
    "         \n",
    "    A = relu(Z)\n",
    "    activation_cache = A\n",
    "         \n",
    "         \n",
    "  assert (A.shape == Z.shape)\n",
    "         \n",
    "  cache = (linear_cache, activation_cache)\n",
    "  \n",
    "  return A,cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A,cache = linear_activation_forward(X,parameters['W1'],parameters['b1'],\"relu\")\n",
    "# print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # A_test= np.array([\n",
    "# #            [3,4,5],\n",
    "# #            [3,4,7],\n",
    "#            [4,2,1]\n",
    "#                   ])\n",
    "# # W_test= np.array([\n",
    "#                 [5,3,1],\n",
    "#                 [6,7,9],\n",
    "#                 [3,5,2]\n",
    "#                        ])\n",
    "# # b_test= ([\n",
    "#           [0],\n",
    "#           [0],\n",
    "#           [0]\n",
    "#              ])\n",
    "# # A,cache = linear_activation_forward(A_test,W_test,b_test,\"relu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# faire une fonction forward sur l layers\n",
    "def forward_layers(X,parameters):\n",
    "    \"\"\"\n",
    "       Function that computes the forward activation for L layers\n",
    "      :param X: input matrix, shape (input_size, number of exemple)\n",
    "      :param parameters: output of initialisation_deep dictionnary of W, b\n",
    "      :type X: matrix of float\n",
    "      :type parameters : dictionary of matrices\n",
    "      :return AL : last post activation value\n",
    "      :return caches : list of caches with every caches of linear_activation_forward \n",
    "      :rtype AL: matrix of float\n",
    "      :rtype caches: list of tuples\n",
    "\n",
    "  \"\"\"\n",
    "    #je crée une  liste de  caches où je vais stoker les valeurs obtenues\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2 # je calcul le nombre de couches par rapport aux nombres de paramètres\n",
    "    #je fais une boucle pour toutes les couches jusqu'à L-1\n",
    "    for l in range (1,L): \n",
    "    #je considère X comme \n",
    "        A_prev= A\n",
    "#       je fais mon calcul pour A0 jusqu'à L-1 ou bien (1 à L)\n",
    "        A,cache = linear_activation_forward(A_prev,parameters[f'W{l}'],parameters[f'b{l}'],\"relu\")\n",
    "       \n",
    "        #je rajoute le cache obtenu dans la liste cache\n",
    "        caches.append(cache)\n",
    "      \n",
    "      #calcul pour la dernière couche:\n",
    "      #je récupère le dernier A qui est sorti de mes couches précédentes et je lui mets une sigmoid \n",
    "      \n",
    "        AL,cache = linear_activation_forward(A,parameters[f'W{l+1}'],parameters[f'b{l+1}'],\"sigmoid\")\n",
    "    \n",
    "        caches.append(cache)\n",
    "   \n",
    "    assert AL.shape == (1,X.shape[1])\n",
    "    \n",
    "    return AL,caches\n",
    "\n",
    " \n",
    "  \n",
    "  \n",
    "      \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AL,caches = forward_layers(X,parameters)\n",
    "# AL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #test sur les petits array\n",
    "\n",
    "# parameters_test={\"W1\":W_test,\n",
    "#                  \"b1\":b_test}\n",
    "\n",
    "# ALtest,caches_test = forward_layers(A_test,parameters_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = X.shape[1]\n",
    "\n",
    "def compute_cost(AL,Y):\n",
    "    \"\"\"\n",
    "        Function that compute the cost \n",
    "        :param AL: probability vector - shape (1,number of examples)\n",
    "        :param Y:  matrix of float\n",
    "        :type AL: matrix\n",
    "        :type Y: array of booleen\n",
    "        :return cost: cost result\n",
    "        :rtype: float \n",
    "    \"\"\"\n",
    "    #je calcule d'abord llog\n",
    "    logprob = (Y * np.log(AL) + (1-Y) * np.log(1-AL))\n",
    "    #ensuite la cost\n",
    "    cost = -(np.sum(logprob))/m\n",
    "    #je veux que l'on me retourne un nombre et non pas un array\n",
    "    cost = np.squeeze(cost)\n",
    "    #être sur que j'ai la cost au bon format\n",
    "    assert(isinstance(cost,float))\n",
    "  \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cost = compute_cost(AL,Y)\n",
    "# print(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "#backward L layer\n",
    "def linear_backward(dZ,cache):\n",
    "    \"\"\"\n",
    "        function that computes the linear backward\n",
    "        :param dZ: gradient of the cost with respect to linear output\n",
    "        :param cache: tuple of value\n",
    "        :return dA_prev: gradient of the cost with respect to activation\n",
    "        :return dW: gradient of the cost with respect to W\n",
    "        :return db: gradient of the cost with respect to b\n",
    "    \"\"\"\n",
    "    #recuperation des valeurs dont j'ai besoin\n",
    "    \n",
    "    A_prev, W, b = cache\n",
    "    m=A_prev.shape[1]\n",
    "    #calcul des dérivées:\n",
    "  \n",
    "    dW = np.dot(dZ,A_prev.T)/m\n",
    "    db = np.sum(dZ,axis=1,keepdims = True)/m\n",
    "    dA_prev= np.dot(W.T,dZ)\n",
    "    \n",
    "    assert(dA_prev.shape == A_prev.shape)\n",
    "    assert(dW.shape == W.shape)\n",
    "    assert(db.shape == b.shape)\n",
    "  \n",
    "    return dA_prev,dW,db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fonction backward pour l'implémentation de la backward si fonction d'activation est une sigmoid ou fonction d'activation est une RELU\n",
    "\n",
    "def linear_activation_backward(dA,cache,activation):\n",
    "    \"\"\"  \n",
    "        function that computes the linear activation backward with sigmoid and relu\n",
    "        :param dA: gradient of the activation for the current layer the l layer\n",
    "        :param cache: tuple of values with the parameters\n",
    "        :param activation: activation function-Relu or Sigmoid\n",
    "        :type dA: numpy array\n",
    "        :type activation: string\n",
    "        :return dA_prev: gradient activation of the l-1 layer-shape = A_prev shape\n",
    "        :return dW: gradient of the cost with respect of the W for the current layer l\n",
    "        :return db: gradient of the cost with respect of the b for the current layer l\n",
    "    \"\"\"\n",
    "    #je récupère de mon cache les paramètres \n",
    "\n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    # je calcule dZ en fonction de l'activation RELU ou Sigmoid\n",
    "    if activation == 'relu':\n",
    "        dZ = derivative_relu(dA, activation_cache)\n",
    "        dA_prev,dW,db = linear_backward (dZ,linear_cache)\n",
    "        \n",
    "    elif activation == 'sigmoid':\n",
    "        dZ = derivative_sigmoid(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ,linear_cache)\n",
    "    \n",
    "    return dA_prev, dW, db\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l_model_backward(AL,Y,caches):\n",
    "    \n",
    "    \"\"\" \n",
    "          function that compute the backward propagation\n",
    "          :param AL: array with the last activation -probability vector \n",
    "          :param Y: array with the label\n",
    "          :param caches: list of caches of all the parameters of relu activation and one cache with all the parameters with sigmoid\n",
    "          :type AL: numpy array\n",
    "          :type Y: vectord\n",
    "          :type caches: python list\n",
    "          :return grads: dictionnary of gradients dW,db,dA\n",
    "          \n",
    "    \"\"\"\n",
    "    grads = {}\n",
    "    L = len(caches) #nombre de couches (correspond aux nombres de caches)\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape) #on reshape Y comme AL pour pouvoir faire les opérations\n",
    "  \n",
    "    #     initialisation de la back propagation pour calculer dAL\n",
    "   \n",
    "    \n",
    "    dAL = -(np.divide(Y,AL)-np.divide(1-Y,1-AL))\n",
    "    \n",
    "  \n",
    "      #calcul des gradients pour la dernière couche L sigmoid \n",
    "    #j'utilise le cache de la dernière couche et je mets tout dans un dictionnaire\n",
    "\n",
    "    current_cache = caches[L-1]\n",
    "    grads[f\"dA{L}\"], grads[f\"dW{L}\"], grads[f\"db{L}\"] = linear_activation_backward(dAL,current_cache, activation =\"sigmoid\")\n",
    "  \n",
    "    #ensuite je fais une boucle pour les autres couches de l= l-2 à l = 0\n",
    "  \n",
    "    for l in reversed (range(L-1)):\n",
    "        \"\"\" \n",
    "#             entrée : la dérivée dA l+1 et le cache de la couche current\n",
    "#             sortie : la dérivée dA l et dWl+1 et dbl+1\n",
    "        \n",
    "#         \"\"\"\n",
    "        print(l)\n",
    "        current_cache = caches[l]\n",
    "        print(grads)\n",
    "    \n",
    "#         #je crée des variables temporaires: dA_prev_temp, dW_temp, db_temp\n",
    "    \n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward (grads[f\"dA{l+1}\"], current_cache, activation = \"relu\")\n",
    "    \n",
    "# #         #je les mets dans le dictionnaire\n",
    "    \n",
    "# #         grads[f\"dA{l}\"] = dA_prev_temp\n",
    "# #         grads[f\"dW{l+1}\"] = dW_temp\n",
    "# #         grads[f\"db{l+1}\"] = db_temp\n",
    "\n",
    "    return grads\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters,grads,learning_rate):\n",
    "    \"\"\"\n",
    "        function that update parameters using the gradient descent\n",
    "        :argument parameters: python dictionary with parameters\n",
    "        :argument grads: python dictionnary with all the gradient\n",
    "        :return parameters: python dictionnar with the updated parameters\n",
    "    \"\"\"\n",
    "    L = len (parameters//2)\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * grads[\"dW\" + str(l+1)]\n",
    "            \n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * grads[\"db\" + str(l + 1)]\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fonction globale qui compute tout (on commence pour faire la forward)\n",
    "\n",
    "def L_layer_model(X,Y, dim_layers, learning_rate = 0.0075,num_iterations = 3000, print_cost = False):\n",
    "    \n",
    "    \"\"\"\n",
    "        :param X: matrix of inputs\n",
    "        :param Y: vector of label\n",
    "        :param layers_dims: list that contains the input size and each layer size\n",
    "        :param learningrate: learning rate for the gradient descent\n",
    "        :param num_iterations: number of iterations of the loop\n",
    "        :param print_cost: decide if it print the cost (True)or not (False) \n",
    "        :type X: numpy matrix\n",
    "        :type Y: numpy array\n",
    "        :type layers_dims: python list\n",
    "        :type learningrate float\n",
    "        :type num_iteration: int\n",
    "        :type printcost : bool\n",
    "        :return \n",
    "    \"\"\"\n",
    "    np.random.seed(1)\n",
    "    cost = []\n",
    "    \n",
    "    #initialisation des paramètres\n",
    "    \n",
    "    parameters = initialisation_deep(dim_layers)\n",
    "    \n",
    "    #boucle de 0 à nombre d'iterations:\n",
    "    \n",
    "    for i in range (0,num_iterations):\n",
    "        \n",
    "        #forward propagation l layers\n",
    "        AL,caches = forward_layers(X,parameters)\n",
    "        \n",
    "        #compute cost\n",
    "        cost = compute_cost(AL,Y)\n",
    "        print(cost)\n",
    "        \n",
    "        #L model backward\n",
    "        grads = l_model_backward(AL,Y,caches)\n",
    "        \n",
    "#         #update parameters\n",
    "#         parameters = update_parameters(parameters,grads,learning_rate)\n",
    "        \n",
    "#         # Print the cost every 100 training example\n",
    "#         if print_cost and i % 100 == 0:\n",
    "#             print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "#         if print_cost and i % 100 == 0:\n",
    "#              costs.append(cost)\n",
    "                \n",
    "#         # plot the cost\n",
    "#         plt.plot(np.squeeze(costs))\n",
    "#         plt.ylabel('cost')\n",
    "#         plt.xlabel('iterations (per hundreds)')\n",
    "#         plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "#         plt.show()\n",
    "\n",
    "    return parameters\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1.shape = 3,2\n",
      "b1.shape = 3, 1 \n",
      "W2.shape = 4,3\n",
      "b2.shape = 4, 1 \n",
      "W3.shape = 5,4\n",
      "b3.shape = 5, 1 \n",
      "W4.shape = 1,5\n",
      "b4.shape = 1, 1 \n",
      "0.6931471805599452\n",
      "4\n",
      "{'dA6': array([[-0.00528278,  0.00528278,  0.00528278, ..., -0.00528278,\n",
      "         0.00528278, -0.00528278],\n",
      "       [ 0.00062001, -0.00062001, -0.00062001, ...,  0.00062001,\n",
      "        -0.00062001,  0.00062001],\n",
      "       [ 0.00762956, -0.00762956, -0.00762956, ...,  0.00762956,\n",
      "        -0.00762956,  0.00762956],\n",
      "       [-0.00303942,  0.00303942,  0.00303942, ..., -0.00303942,\n",
      "         0.00303942, -0.00303942],\n",
      "       [ 0.0016745 , -0.0016745 , -0.0016745 , ...,  0.0016745 ,\n",
      "        -0.0016745 ,  0.0016745 ]]), 'dW6': array([[0., 0., 0., 0., 0.]]), 'db6': array([[0.02350037]])}\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'dA3'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-210-e6bc542e6cc8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mparameters\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mL_layer_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdim_layers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlearning_rate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.0075\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnum_iterations\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprint_cost\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-207-b39f64c3d70c>\u001b[0m in \u001b[0;36mL_layer_model\u001b[1;34m(X, Y, dim_layers, learning_rate, num_iterations, print_cost)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[1;31m#L model backward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m         \u001b[0mgrads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ml_model_backward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAL\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcaches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;31m#         #update parameters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-209-59a6ae22d33d>\u001b[0m in \u001b[0;36ml_model_backward\u001b[1;34m(AL, Y, caches)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;31m#         #je crée des variables temporaires: dA_prev_temp, dW_temp, db_temp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m         \u001b[0mdA_prev_temp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdW_temp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdb_temp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlinear_activation_backward\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34mf\"dA{l-1}\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcurrent_cache\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"relu\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;31m# #         #je les mets dans le dictionnaire\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'dA3'"
     ]
    }
   ],
   "source": [
    "parameters = L_layer_model(X,Y,dim_layers,learning_rate = 0.0075,num_iterations = 5, print_cost = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'grads' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-194-ce5a7042628c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mgrads\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'grads' is not defined"
     ]
    }
   ],
   "source": [
    "grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "gist": {
   "data": {
    "description": "neural_l_layerfromscratchV2.ipynb",
    "public": false
   },
   "id": ""
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
