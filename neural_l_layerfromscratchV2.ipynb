{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_Xor(nb_features=400):\n",
    "  \n",
    "    \"\"\"\n",
    "        Function that creates XOR dataset\n",
    "        :param nb_features: number of features\n",
    "        :type nb_features : int\n",
    "        :return: X  input numpy array shape(2,400)\n",
    "        :rtype: numpy array\n",
    "        :return: Y true-false labels numpy array shape(400,1)\n",
    "        :rtype: numpy array\n",
    "    \n",
    "      \"\"\"\n",
    "    X = np.random.randint(2, size = (2,nb_features))\n",
    "    Y = np.logical_xor(X[0,:],X[1,:])\n",
    "    print(Y)\n",
    "  \n",
    "    #reshape Y\n",
    "  \n",
    "    Y = Y.reshape(1,nb_features)\n",
    "                        \n",
    "    print(f\"X.shape = {X.shape}\")\n",
    "    print(f\"Y.shape = {Y.shape}\")\n",
    "                        \n",
    "    return X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False  True  True False  True  True False False  True  True  True False\n",
      " False False False False  True  True  True False False  True  True False\n",
      "  True  True False False False  True  True  True  True False  True  True\n",
      " False  True  True False False  True False False False False False False\n",
      "  True False  True False  True False  True False False  True False False\n",
      "  True  True False False  True False False  True  True  True False False\n",
      "  True False  True  True  True False False False False  True  True False\n",
      " False False  True False False False  True  True False False False False\n",
      " False  True False  True False  True False  True False False  True  True\n",
      " False  True  True False False  True  True False False  True False  True\n",
      "  True  True False  True False False  True False  True False False False\n",
      "  True False  True False False False  True  True  True  True  True False\n",
      " False  True False  True  True False  True False  True  True False False\n",
      " False False False False False False  True False  True False  True  True\n",
      "  True  True  True False  True False  True  True False  True False False\n",
      "  True  True  True False False False  True  True  True False  True False\n",
      " False  True False False  True False False  True False False  True False\n",
      "  True False False  True False  True False False  True False False False\n",
      "  True False  True  True  True  True  True False False  True  True  True\n",
      "  True  True  True False  True  True False  True False False  True False\n",
      "  True False  True  True False False  True False False False  True False\n",
      " False  True  True False  True False  True  True  True False  True False\n",
      " False  True False False  True  True  True  True  True False False False\n",
      " False False False  True False  True  True False  True False False False\n",
      " False False  True  True  True False False False False False False False\n",
      " False  True  True  True False  True  True False False False  True False\n",
      " False False False False False False  True False False False False  True\n",
      " False False False False False  True False False False False False False\n",
      "  True  True False  True  True False False False False  True False False\n",
      " False  True False False  True  True  True  True  True False  True False\n",
      " False  True False  True  True False  True  True  True  True  True False\n",
      " False  True  True  True False  True  True  True  True  True  True  True\n",
      " False  True  True  True False False  True  True  True  True  True  True\n",
      " False  True  True False]\n",
      "X.shape = (2, 400)\n",
      "Y.shape = (1, 400)\n"
     ]
    }
   ],
   "source": [
    " X,Y = create_Xor()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "  \"\"\"\n",
    "      Function that implement sigmoid\n",
    "      :param Z: input value\n",
    "      :type Z: float, array\n",
    "      :return s: sigmoid result of Z \n",
    "      :rtype s:  float or array\n",
    "  \"\"\"\n",
    "  \n",
    "  s = 1/(1+(np.exp(-Z)))\n",
    "  # sigmoid(0) == 0.5\n",
    "  \n",
    "  assert(0.5 == 1/(1+(np.exp(0))))\n",
    "  \n",
    "  return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8807970779778823"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test sigmoid\n",
    "\n",
    "sigmoid(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5       , 0.88079708])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sigmoid d'un array\n",
    "sigmoid_array= sigmoid(np.array([0,2]))\n",
    "sigmoid_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "    \"\"\"\n",
    "          Function that implement relu\n",
    "          :param Z: input value\n",
    "          :type Z: float, array\n",
    "          :return: relu result of Z \n",
    "          :rtype: float or array\n",
    "      \"\"\"\n",
    "  \n",
    " #   Z>0 c'est un test si Z>0 alors on va avoir 1 sinon on va avoir 0\n",
    "\n",
    "    r = Z * (Z>0)\n",
    "    \n",
    "    #r = np.maximum(0,Z)\n",
    "   \n",
    "    assert(0.5 == 0.5 * (0.5>0))\n",
    "    assert(0   == -1  * (-1>0))\n",
    "  \n",
    "  \n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivative_sigmoid(dA,activation_cache):\n",
    "    \"\"\"\n",
    "      Function that implement the derivative of sigmoid\n",
    "      :param Z: input value\n",
    "      :type Z: float, array\n",
    "      :return: derivative result of Z \n",
    "      :rtype: float or array\n",
    "    \"\"\"\n",
    "    Z = activation_cache\n",
    "#     print(\"Z.shape = \", Z.shape, \"dA.shape = \", dA.shape)\n",
    "    s = 1/(1+np.exp(-Z))\n",
    "    dZ= dA * s *(1-s)\n",
    "#     print(\"dZ.shape = \", dZ.shape, \"s.shape = \", s.shape)\n",
    "    assert(0.25 == sigmoid (0) * (1-sigmoid(0)))\n",
    "  \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dZ = derivative_sigmoid(45)\n",
    "# dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def derivative_relu(dA,activation_cache):\n",
    "#   \"\"\"\n",
    "#       Function that implement the derivative of relu\n",
    "#       :param Z: input value\n",
    "#       :type Z: float, array\n",
    "#       :return: derivative result of Z \n",
    "#       :rtype: float or array\n",
    "      \n",
    "#   \"\"\"\n",
    "  \n",
    "#   dZ= 1 * (Z>0)\n",
    "  \n",
    "#   assert (0 == 1 * (-1>0) )\n",
    "#   assert (1 == 1 * (1>0) )\n",
    "  \n",
    "#   return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivative_relu(dA, activation_cache):\n",
    "    \n",
    "    Z = activation_cache\n",
    "    dZ = np.array(dA,copy = True)\n",
    "    dZ[Z<= 0] = 0\n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dZ= derivative_relu(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialisation_deep(dim_layers):\n",
    "  \n",
    "    \"\"\" \n",
    "      Function that initialize weights and biais for each layer\n",
    "      :param dim_layers: list of each layer\n",
    "      :type dim_layers: pyhon list\n",
    "      :return parameters dictionnary with W1,b1,......WL,bL\n",
    "      :rtype: python dictionnary\n",
    "      Wl --- weight matrix of shape (dim_layers[l],dim_layers[1-l])\n",
    "      b1 --- weight matrix of shape (dim_layer[l],dim_layers[1-l])\n",
    "    \"\"\"\n",
    "    np.random.seed(3)\n",
    "    parameters={}\n",
    "    \n",
    "    L = len(dim_layers)\n",
    "    \n",
    "    for l in range(1,L):\n",
    "   \n",
    "        parameters[f'W{l}']= np.random.randn(dim_layers[l], dim_layers[l-1])*0.01\n",
    "\n",
    "        parameters[f'b{l}'] = np.zeros((dim_layers[l],1))\n",
    "    \n",
    "#     print(f'W{l}.shape = {dim_layers[l]},{dim_layers[l-1]}')\n",
    "#     print(f\"b{l}.shape = {dim_layers[l]}, 1 \")\n",
    "\n",
    "    assert (parameters[f\"W{l}\"].shape == (dim_layers[l],dim_layers[l-1]))\n",
    "    assert (parameters[f\"b{l}\"].shape == (dim_layers[l],1))\n",
    "    \n",
    "    return parameters\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'W1': array([[ 0.01788628,  0.0043651 ],\n",
       "        [ 0.00096497, -0.01863493],\n",
       "        [-0.00277388, -0.00354759]]), 'b1': array([[0.],\n",
       "        [0.],\n",
       "        [0.]]), 'W2': array([[-0.00082741, -0.00627001, -0.00043818],\n",
       "        [-0.00477218, -0.01313865,  0.00884622],\n",
       "        [ 0.00881318,  0.01709573,  0.00050034],\n",
       "        [-0.00404677, -0.0054536 , -0.01546477]]), 'b2': array([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]]), 'W3': array([[ 0.00982367, -0.01101068, -0.01185047, -0.0020565 ]]), 'b3': array([[0.]])}"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim_layers= [2,3,4,1]\n",
    "parameters = initialisation_deep(dim_layers)\n",
    "\n",
    "parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_forward (A_prev ,W , b, activation):\n",
    "    \"\"\"\n",
    "      function that both computes preactivation and activation \n",
    "      :param A_prev: previews activation matrix (for the first layer it is X )\n",
    "      :param W: weight matrix for the current layer\n",
    "      :param b: biais matrix for the current layer\n",
    "      :param activation: choice of activation function eg: sigmoid , relu\n",
    "      :type A_prev : matrix of float\n",
    "      :type W: matrix of float\n",
    "      :type b: matrix of float\n",
    "      :return: A matrix of activation\n",
    "      :cache: tuple of (linear_cache,activation_cache) \n",
    "      \"\"\"\n",
    "\n",
    "    Z = np.dot(W,A_prev)+b\n",
    "\n",
    "    linear_cache = (A_prev,W,b)\n",
    "  \n",
    "  \n",
    "    assert(Z.shape == (W.shape[0], A_prev.shape[1]))\n",
    "  \n",
    "    if activation == \"sigmoid\":\n",
    "         \n",
    "        A = sigmoid(Z)\n",
    "        activation_cache = A\n",
    "    \n",
    "    elif activation == \"relu\":\n",
    "         \n",
    "        A = relu(Z)\n",
    "        activation_cache = A\n",
    "         \n",
    "         \n",
    "    assert (A.shape == Z.shape)\n",
    "         \n",
    "    cache = (linear_cache, activation_cache)\n",
    "  \n",
    "    return A,cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A,cache = linear_activation_forward(X,parameters['W1'],parameters['b1'],\"relu\")\n",
    "# print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # A_test= np.array([\n",
    "# #            [3,4,5],\n",
    "# #            [3,4,7],\n",
    "#            [4,2,1]\n",
    "#                   ])\n",
    "# # W_test= np.array([\n",
    "#                 [5,3,1],\n",
    "#                 [6,7,9],\n",
    "#                 [3,5,2]\n",
    "#                        ])\n",
    "# # b_test= ([\n",
    "#           [0],\n",
    "#           [0],\n",
    "#           [0]\n",
    "#              ])\n",
    "# # A,cache = linear_activation_forward(A_test,W_test,b_test,\"relu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# faire une fonction forward sur l layers\n",
    "def forward_layers(X,parameters):\n",
    "    \"\"\"\n",
    "       Function that computes the forward activation for L layers\n",
    "      :param X: input matrix, shape (input_size, number of exemple)\n",
    "      :param parameters: output of initialisation_deep dictionnary of W, b\n",
    "      :type X: matrix of float\n",
    "      :type parameters : dictionary of matrices\n",
    "      :return AL : last post activation value\n",
    "      :return caches : list of caches with every caches of linear_activation_forward \n",
    "      :rtype AL: matrix of float\n",
    "      :rtype caches: list of tuples\n",
    "\n",
    "  \"\"\"\n",
    "    #je crée une  liste de  caches où je vais stoker les valeurs obtenues\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2 # je calcul le nombre de couches par rapport aux nombres de paramètres\n",
    "    #je fais une boucle pour toutes les couches jusqu'à L-1\n",
    "    for l in range (1,L): \n",
    "    #je considère X comme \n",
    "        A_prev= A\n",
    "#       je fais mon calcul pour A0 jusqu'à L-1 ou bien (1 à L)\n",
    "        A,cache = linear_activation_forward(A_prev,parameters[f'W{l}'],parameters[f'b{l}'],\"sigmoid\")\n",
    "       \n",
    "        #je rajoute le cache obtenu dans la liste cache\n",
    "        caches.append(cache)\n",
    "      \n",
    "      #calcul pour la dernière couche:\n",
    "      #je récupère le dernier A qui est sorti de mes couches précédentes et je lui mets une sigmoid \n",
    "      \n",
    "    AL,cache = linear_activation_forward(A,parameters[f'W{l+1}'],parameters[f'b{l+1}'],\"sigmoid\")\n",
    "    caches.append(cache)\n",
    "   \n",
    "    assert AL.shape == (1,X.shape[1])\n",
    "    \n",
    "    return AL,caches\n",
    "\n",
    " \n",
    "  \n",
    "  \n",
    "      \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AL,caches = forward_layers(X,parameters)\n",
    "# AL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #test sur les petits array\n",
    "\n",
    "# parameters_test={\"W1\":W_test,\n",
    "#                  \"b1\":b_test}\n",
    "\n",
    "# ALtest,caches_test = forward_layers(A_test,parameters_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = X.shape[1]\n",
    "\n",
    "def compute_cost(AL,Y):\n",
    "    \"\"\"\n",
    "        Function that compute the cost \n",
    "        :param AL: probability vector - shape (1,number of examples)\n",
    "        :param Y:  matrix of float\n",
    "        :type AL: matrix\n",
    "        :type Y: array of booleen\n",
    "        :return cost: cost result\n",
    "        :rtype: float \n",
    "    \"\"\"\n",
    "    #je calcule d'abord llog\n",
    "    logprob = (Y * np.log(AL) + (1-Y) * np.log(1-AL))\n",
    "    #ensuite la cost\n",
    "    cost = -(np.sum(logprob))/m\n",
    "    #je veux que l'on me retourne un nombre et non pas un array\n",
    "    cost = np.squeeze(cost)\n",
    "    #être sur que j'ai la cost au bon format\n",
    "    assert(isinstance(cost,float))\n",
    "  \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cost_test = compute_cost(Y,Y)\n",
    "# print(cost_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "#backward L layer\n",
    "def linear_backward(dZ,cache):\n",
    "    \"\"\"\n",
    "        function that computes the linear backward\n",
    "        :param dZ: gradient of the cost with respect to linear output\n",
    "        :param cache: tuple of value\n",
    "        :return dA_prev: gradient of the cost with respect to activation\n",
    "        :return dW: gradient of the cost with respect to W\n",
    "        :return db: gradient of the cost with respect to b\n",
    "    \"\"\"\n",
    "    #recuperation des valeurs dont j'ai besoin\n",
    "    \n",
    "    A_prev, W, b = cache\n",
    "    m=A_prev.shape[1]\n",
    "    #calcul des dérivées:\n",
    "  \n",
    "    dW = np.dot(dZ,A_prev.T)/m\n",
    "    db = np.sum(dZ,axis=1,keepdims = True)/m\n",
    "    dA_prev= np.dot(W.T,dZ)\n",
    "    \n",
    "    assert(dA_prev.shape == A_prev.shape)\n",
    "    assert(dW.shape == W.shape)\n",
    "    assert(db.shape == b.shape)\n",
    "  \n",
    "    return dA_prev,dW,db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fonction backward pour l'implémentation de la backward si fonction d'activation est une sigmoid ou fonction d'activation est une RELU\n",
    "\n",
    "def linear_activation_backward(dA,cache,activation):\n",
    "    \"\"\"  \n",
    "        function that computes the linear activation backward with sigmoid and relu\n",
    "        :param dA: gradient of the activation for the current layer the l layer\n",
    "        :param cache: tuple of values with the parameters\n",
    "        :param activation: activation function-Relu or Sigmoid\n",
    "        :type dA: numpy array\n",
    "        :type activation: string\n",
    "        :return dA_prev: gradient activation of the l-1 layer-shape = A_prev shape\n",
    "        :return dW: gradient of the cost with respect of the W for the current layer l\n",
    "        :return db: gradient of the cost with respect of the b for the current layer l\n",
    "    \"\"\"\n",
    "    #je récupère de mon cache les paramètres \n",
    "\n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    # je calcule dZ en fonction de l'activation RELU ou Sigmoid\n",
    "    if activation == 'relu':\n",
    "        dZ = derivative_relu(dA, activation_cache)\n",
    "        dA_prev,dW,db = linear_backward (dZ,linear_cache)\n",
    "        \n",
    "    elif activation == 'sigmoid':\n",
    "        dZ = derivative_sigmoid(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ,linear_cache)\n",
    "    \n",
    "    return dA_prev, dW, db\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l_model_backward(AL,Y,caches):\n",
    "    \n",
    "    \"\"\" \n",
    "          function that compute the backward propagation\n",
    "          :param AL: array with the last activation -probability vector \n",
    "          :param Y: array with the label\n",
    "          :param caches: list of caches of all the parameters of relu activation and one cache with all the parameters with sigmoid\n",
    "          :type AL: numpy array\n",
    "          :type Y: vectord\n",
    "          :type caches: python list\n",
    "          :return grads: dictionnary of gradients dW,db,dA\n",
    "          \n",
    "    \"\"\"\n",
    "    grads = {}\n",
    "#     print(\"initialisation de grad\")\n",
    "    L = len(caches) #nombre de couches (correspond aux nombres de caches)\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape) #on reshape Y comme AL pour pouvoir faire les opérations\n",
    "#     print(\"L = \", L, \" m = \",m, \" Y = \",Y)\n",
    "    #     initialisation de la back propagation pour calculer dAL\n",
    "   \n",
    "    \n",
    "    dAL = -(np.divide(Y,AL)-np.divide(1-Y,1-AL))\n",
    "#     print(\"dAL=\", dAL)\n",
    "    \n",
    "  \n",
    "      #calcul des gradients pour la dernière couche L sigmoid \n",
    "    #j'utilise le cache de la dernière couche et je mets tout dans un dictionnaire\n",
    "\n",
    "    current_cache = caches[L-1]\n",
    "#     print(\"current_cache = \", current_cache)\n",
    "    grads[f\"dA{L-1}\"], grads[f\"dW{L}\"], grads[f\"db{L}\"] = linear_activation_backward(dAL,current_cache, activation =\"sigmoid\")\n",
    "#     print([f\"dA{L-1}\"], grads[f\"dW{L}\"], grads[f\"db{L}\"])\n",
    "  \n",
    "    #ensuite je fais une boucle pour les autres couches de l= l-2 à l = 0\n",
    "  \n",
    "    for l in reversed(range(L-1)):\n",
    "        \"\"\" \n",
    "#             entrée : la dérivée dA l+1 et le cache de la couche current\n",
    "#             sortie : la dérivée dA l et dWl+1 et dbl+1\n",
    "        \n",
    "#         \"\"\"\n",
    "        \n",
    "        current_cache = caches[l]\n",
    "        \n",
    "#         print(\"l = \", l , \"current_cache W =\", current_cache[0][1].shape)\n",
    "#         #je crée des variables temporaires: dA_prev_temp, dW_temp, db_temp\n",
    "    \n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward (grads[f\"dA{l+1}\"], current_cache, activation = \"sigmoid\")\n",
    "    \n",
    "# #         #je les mets dans le dictionnaire\n",
    "    \n",
    "        grads[f\"dA{l}\"] = dA_prev_temp\n",
    "        grads[f\"dW{l+1}\"] = dW_temp\n",
    "        grads[f\"db{l+1}\"] = db_temp\n",
    "\n",
    "#         print (\"da\", l, grads[f\"dA{l}\"].shape)\n",
    "\n",
    "        \n",
    "    return grads\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters,grads,learning_rate):\n",
    "    \"\"\"\n",
    "        function that update parameters using the gradient descent\n",
    "        :argument parameters: python dictionary with parameters\n",
    "        :argument grads: python dictionnary with all the gradient\n",
    "        :return parameters: python dictionnar with the updated parameters\n",
    "    \"\"\"\n",
    "    L = len (parameters)//2\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * grads[\"dW\" + str(l+1)]\n",
    "            \n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * grads[\"db\" + str(l + 1)]\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fonction globale qui compute tout (on commence pour faire la forward)\n",
    "\n",
    "def L_layer_model(X,Y, dim_layers, learning_rate = 0.0075,num_iterations = 3000, print_cost = False):\n",
    "    \n",
    "    \"\"\"\n",
    "        :param X: matrix of inputs\n",
    "        :param Y: vector of label\n",
    "        :param layers_dims: list that contains the input size and each layer size\n",
    "        :param learningrate: learning rate for the gradient descent\n",
    "        :param num_iterations: number of iterations of the loop\n",
    "        :param print_cost: decide if it print the cost (True)or not (False) \n",
    "        :type X: numpy matrix\n",
    "        :type Y: numpy array\n",
    "        :type layers_dims: python list\n",
    "        :type learningrate float\n",
    "        :type num_iteration: int\n",
    "        :type printcost : bool\n",
    "        :return \n",
    "    \"\"\"\n",
    "    np.random.seed(1)\n",
    "    costs = []\n",
    "    \n",
    "    #initialisation des paramètres\n",
    "    \n",
    "    parameters = initialisation_deep(dim_layers)\n",
    "    \n",
    "    #boucle de 0 à nombre d'iterations:\n",
    "    \n",
    "    for i in range (0,num_iterations):\n",
    "        \n",
    "        #forward propagation l layers\n",
    "        AL,caches = forward_layers(X,parameters)\n",
    "        \n",
    "        #compute cost\n",
    "        cost = compute_cost(AL,Y)\n",
    "#         print(cost)\n",
    "        \n",
    "        #L model backward\n",
    "        grads = l_model_backward(AL,Y,caches)\n",
    "        \n",
    "        #update parameters\n",
    "        parameters = update_parameters(parameters,grads,learning_rate)\n",
    "        \n",
    "    # Print the cost every 100 training example\n",
    "        if print_cost and i % 1000 == 0:\n",
    "            print(cost)\n",
    "#             print (f\"Cost after iteration {i}{cost}\")\n",
    "        if print_cost and i % 1000 == 0:\n",
    "            costs.append(cost)\n",
    "                \n",
    "#     plot the cost\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per hundreds)')\n",
    "    plt.title(f\"Learning rate ={learning_rate}\")\n",
    "    plt.show()\n",
    "\n",
    "    return parameters\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6928352329162999\n",
      "0.6918985896151412\n",
      "0.6918966632487851\n",
      "0.6918966593344215\n",
      "0.6918966593264687\n",
      "0.6918966593264494\n",
      "0.6918966593264457\n",
      "0.6918966593264426\n",
      "0.6918966593264392\n",
      "0.6918966593264357\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "parameters = L_layer_model(X,Y,dim_layers,learning_rate = 0.0075 ,num_iterations = 10000, print_cost = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.async-def-wrapper.<locals>.predict(parameters, X)>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# def predict(parameters,X):\n",
    "  \n",
    "#   # je fais passer la forward\n",
    "#     AL, caches = forward_layers(X,parameters)\n",
    "#     predict = (AL>0.5)\n",
    "# return predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total_size = X.shape[1]\n",
    "# train_size = int(0.8 * total_size)\n",
    "# test_size  = total_size - train_size \n",
    "# X_train, Y_train = (X[:train_size], Y[:train_size])\n",
    "\n",
    "\n",
    "# predictions_train = predict(train_x, train_y, parameters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, y, parameters):\n",
    "    \"\"\"\n",
    "    This function is used to predict the results of a  L-layer neural network.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data set of examples you would like to label\n",
    "    parameters -- parameters of the trained model\n",
    "    \n",
    "    Returns:\n",
    "    p -- predictions for the given dataset X\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    n = len(parameters) // 2 # number of layers in the neural network\n",
    "    p = np.zeros((1,m))\n",
    "    \n",
    "    # Forward propagation\n",
    "    probas, caches = forward_layers(X, parameters)\n",
    "\n",
    "    \n",
    "    # convert probas to 0/1 predictions\n",
    "    for i in range(0, probas.shape[1]):\n",
    "        if probas[0,i] > 0.5:\n",
    "            p[0,i] = 1\n",
    "        else:\n",
    "            p[0,i] = 0\n",
    "    \n",
    "    #print results\n",
    "    #print (\"predictions: \" + str(p))\n",
    "    #print (\"true labels: \" + str(y))\n",
    "    print(\"Accuracy: \"  + str(np.sum((p == y)/m)))\n",
    "        \n",
    "    return p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False  True  True False False False False  True False  True  True  True\n",
      "  True  True False False  True False  True False False  True  True  True\n",
      "  True False False  True  True  True False False  True  True  True False\n",
      " False  True  True False False  True  True  True False  True  True  True\n",
      " False False  True False False False False  True False False  True False\n",
      " False  True False  True  True False  True  True  True  True False False\n",
      " False False False  True False False  True  True  True False  True  True\n",
      " False False  True  True  True False  True False  True  True  True False\n",
      "  True  True False False]\n",
      "X.shape = (2, 100)\n",
      "Y.shape = (1, 100)\n"
     ]
    }
   ],
   "source": [
    "X_test, Y_test = create_Xor(nb_features=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4700000000000001\n"
     ]
    }
   ],
   "source": [
    " predictions_train=predict(X_test,Y_test,parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "gist": {
   "data": {
    "description": "neural_l_layerfromscratchV2.ipynb",
    "public": false
   },
   "id": ""
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
